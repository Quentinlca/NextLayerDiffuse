{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470500f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"QLeca/modular_characters_hairs_RGB\", split=\"train\", cache_dir=\"cache/datasets\")\n",
    "vocab = dataset[\"prompt\"]  # type: ignore\n",
    "vocab = list(dict.fromkeys(dataset[\"prompt\"]))  # type: ignore\n",
    "vocab = sorted(vocab)  # type: ignore\n",
    "vocab = dict(zip(vocab, range(len(vocab))))\n",
    "\n",
    "import json\n",
    "with open(\"vocab.json\", \"w\") as f:\n",
    "    json.dump(vocab, f, indent=2)\n",
    "f.close()\n",
    "print(\"Vocabulary saved to vocab.json, size:\", len(vocab))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5e4526",
   "metadata": {},
   "outputs": [],
   "source": [
    "from layer_diffuse.data_loaders import ModularCharatersDataLoader\n",
    "from layer_diffuse.models import DDIMNextTokenV1_Refactored\n",
    "import json\n",
    "# pipeline = DDIMNextTokenV1_Refactored.DDIMNextTokenV1PipelineRefactored()\n",
    "vocab_file = \"layer_diffuse/vocab.json\"\n",
    "with open(vocab_file, 'r') as f:\n",
    "    vocab = json.load(f)\n",
    "dataloader = ModularCharatersDataLoader.get_modular_char_dataloader(dataset_name='QLeca/modular_characters_v3',\n",
    "                                                                            split='train',\n",
    "                                                                            image_size=128,\n",
    "                                                                            batch_size=8,\n",
    "                                                                            shuffle=True,\n",
    "                                                                            streaming=True,\n",
    "                                                                            conversionRGBA=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846dfecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "def show_image_grid(input_images, output_images, target_images):\n",
    "    output_images = (output_images * 0.5 + 0.5).clamp(0, 1).cpu()\n",
    "    input_images = (input_images * 0.5 + 0.5).clamp(0, 1).cpu()\n",
    "    target_images = (target_images * 0.5  + 0.5).clamp(0, 1).cpu()\n",
    "    concat = torch.concat([input_images, output_images, target_images])\n",
    "    grid = make_grid(concat, nrow=input_images.shape[0])\n",
    "    img = torchvision.transforms.ToPILImage()(grid)\n",
    "    display(img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a363286",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in dataloader:\n",
    "    input_images = batch['input']\n",
    "    target_images = batch['target']\n",
    "    labels = batch['label']\n",
    "    outputs = pipeline(input_images=input_images, \n",
    "                       class_labels=labels,\n",
    "                       num_inference_steps=50)\n",
    "    show_image_grid(input_images, outputs, target_images)\n",
    "    break    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709b67b1",
   "metadata": {},
   "source": [
    "# Inference test Widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73151658",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive Model Selection Widget\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "import wandb\n",
    "import json\n",
    "import os\n",
    "from layer_diffuse.models import DDIMNextTokenV1_Refactored, DDPMNextTokenV1, DDPMNextTokenV2, DDPMNextTokenV3_Refactored, BaseNextTokenPipeline\n",
    "from layer_diffuse.data_loaders import ModularCharatersDataLoader\n",
    "\n",
    "# Global variables to store current configuration\n",
    "current_pipeline = None\n",
    "current_dataloader = None\n",
    "current_config = {}\n",
    "\n",
    "# Available model types\n",
    "MODEL_TYPES = {\n",
    "    \"DDIM Next Token V1 (Refactored)\": DDIMNextTokenV1_Refactored.DDIMNextTokenV1PipelineRefactored,\n",
    "    \"DDPM Next Token V1\": DDPMNextTokenV1.DDPMNextTokenV1Pipeline,\n",
    "    \"DDPM Next Token V2\": DDPMNextTokenV2.DDPMNextTokenV2Pipeline,\n",
    "    \"DDPM Next Token V3 (Refactored)\": DDPMNextTokenV3_Refactored.DDPMNextTokenV3Pipeline,\n",
    "}\n",
    "\n",
    "HF_REPOSITORIES = {\n",
    "    \"DDIM Next Token V1 (Refactored)\": \"QLeca/DDIMNextTokenV1\",\n",
    "    \"DDPM Next Token V1\": \"QLeca/DDPMNextTokenV1\",\n",
    "    \"DDPM Next Token V2\": \"QLeca/DDPMNextTokenV2\",\n",
    "    \"DDPM Next Token V3 (Refactored)\": \"QLeca/DDPMNextTokenV3\",\n",
    "}\n",
    "\n",
    "WANDB_PROJECTS = {\"DDIM Next Token V1 (Refactored)\": \"ddim-next-token-v1\",\n",
    "                  \"DDPM Next Token V1\": \"ddpm-next-token-v1\",\n",
    "                  \"DDPM Next Token V2\": \"ddpm-next-token-v2\",\n",
    "                  \"DDPM Next Token V3 (Refactored)\": \"ddpm-next-token-v3\"\n",
    "                  }\n",
    "\n",
    "# Store available versions for each model type\n",
    "available_versions = {}\n",
    "\n",
    "# Available datasets (will be auto-detected from wandb runs)\n",
    "DATASETS = {}\n",
    "\n",
    "# Create widgets\n",
    "model_dropdown = widgets.Dropdown(\n",
    "    options=list(MODEL_TYPES.keys()),\n",
    "    value=list(MODEL_TYPES.keys())[0],\n",
    "    description='Model Type:',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout=widgets.Layout(width='400px')\n",
    ")\n",
    "\n",
    "dataset_dropdown = widgets.Dropdown(\n",
    "    options=['Select a run first'],\n",
    "    value='Select a run first',\n",
    "    description='Dataset:',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout=widgets.Layout(width='400px'),\n",
    "    disabled=True\n",
    ")\n",
    "\n",
    "run_name_dropdown = widgets.Dropdown(\n",
    "    options=['Select a model type first'],\n",
    "    value='Select a model type first',\n",
    "    description='Run Name:',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout=widgets.Layout(width='400px'),\n",
    "    disabled=True\n",
    ")\n",
    "\n",
    "epoch_dropdown = widgets.Dropdown(\n",
    "    options=['Select a run first'],\n",
    "    value='Select a run first',\n",
    "    description='Epoch:',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout=widgets.Layout(width='400px'),\n",
    "    disabled=True\n",
    ")\n",
    "\n",
    "# Additional dataset parameters\n",
    "batch_size_slider = widgets.IntSlider(\n",
    "    value=8,\n",
    "    min=1,\n",
    "    max=32,\n",
    "    step=1,\n",
    "    description='Batch Size:',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout=widgets.Layout(width='400px')\n",
    ")\n",
    "\n",
    "image_size_dropdown = widgets.Dropdown(\n",
    "    options=[64, 128, 256, 512],\n",
    "    value=128,\n",
    "    description='Image Size:',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout=widgets.Layout(width='400px')\n",
    ")\n",
    "\n",
    "streaming_checkbox = widgets.Checkbox(\n",
    "    value=True,\n",
    "    description='Use Streaming',\n",
    "    style={'description_width': 'initial'},\n",
    ")\n",
    "\n",
    "# Output widget for status messages\n",
    "output = widgets.Output()\n",
    "\n",
    "# Function to get dataset info from wandb run\n",
    "def get_dataset_from_run(model_type, run_name):\n",
    "    \"\"\"Get dataset information from a specific wandb run\"\"\"\n",
    "    try:\n",
    "        import wandb\n",
    "        api = wandb.Api()\n",
    "        \n",
    "        # Get the repository for this model type\n",
    "        repo = WANDB_PROJECTS[model_type]\n",
    "        project_name = repo.split('/')[-1]  # Extract project name from repo\n",
    "        \n",
    "        # Find the specific run\n",
    "        runs = api.runs(project_name)\n",
    "        target_run = None\n",
    "        \n",
    "        for run in runs:\n",
    "            if run.name == run_name:\n",
    "                target_run = run\n",
    "                break\n",
    "        \n",
    "        if target_run is None:\n",
    "            return None\n",
    "            \n",
    "        # Get dataset information from run config\n",
    "        config = target_run.config\n",
    "        dataset_name = config.get('dataset', {}).get('name', None)\n",
    "        \n",
    "        if dataset_name:\n",
    "            return dataset_name\n",
    "        else:\n",
    "            return None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error getting dataset from run {run_name}: {e}\")\n",
    "        return None\n",
    "# Function to get available runs and epochs from pipeline.list_versions()\n",
    "def get_available_versions(model_type):\n",
    "    \"\"\"Get available runs and epochs for a specific model type\"\"\"\n",
    "    try:\n",
    "        versions = BaseNextTokenPipeline.BaseNextTokenPipeline.get_model_versions(HF_REPOSITORIES[model_type])\n",
    "        return versions\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting versions for {model_type}: {e}\")\n",
    "        return []\n",
    "\n",
    "def get_scheduler_config_from_run(model_type, run_name):\n",
    "    \"\"\"Get scheduler configuration from a specific wandb run\"\"\"\n",
    "    try:\n",
    "        import wandb\n",
    "        api = wandb.Api()\n",
    "        \n",
    "        # Get the repository for this model type\n",
    "        repo = WANDB_PROJECTS[model_type]\n",
    "        project_name = repo.split('/')[-1]  # Extract project name from repo\n",
    "        \n",
    "        # Find the specific run\n",
    "        runs = api.runs(project_name)\n",
    "        target_run = None\n",
    "        \n",
    "        for run in runs:\n",
    "            if run.name == run_name:\n",
    "                target_run = run\n",
    "                break\n",
    "        \n",
    "        if target_run is None:\n",
    "            return None\n",
    "            \n",
    "        # Get scheduler config from run config\n",
    "        config = target_run.config\n",
    "        scheduler_config = config.get('scheduler_config', None)\n",
    "        \n",
    "        return scheduler_config\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error getting scheduler config from run {run_name}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to update available runs when model type changes\n",
    "def update_runs_for_model(change=None):\n",
    "    \"\"\"Update available runs dropdown based on selected model type\"\"\"\n",
    "    model_type = model_dropdown.value\n",
    "    \n",
    "    with output:\n",
    "        clear_output()\n",
    "        print(f\"üîÑ Loading available runs for {model_type}...\")\n",
    "    \n",
    "    try:\n",
    "        # Get versions for the selected model type\n",
    "        versions = get_available_versions(model_type)\n",
    "        available_versions[model_type] = versions\n",
    "        \n",
    "        if versions:\n",
    "            run_names = [v['name'] for v in versions]\n",
    "            run_name_dropdown.options = run_names\n",
    "            run_name_dropdown.value = run_names[0]\n",
    "            run_name_dropdown.disabled = False\n",
    "            \n",
    "            # Update epochs for the first run\n",
    "            update_epochs_for_run({'new': run_names[0]})\n",
    "            \n",
    "            # Update dataset for the first run\n",
    "            update_dataset_for_run({'new': run_names[0]})\n",
    "            \n",
    "            with output:\n",
    "                clear_output()\n",
    "                print(f\"‚úÖ Found {len(versions)} runs for {model_type}\")\n",
    "                for i, version in enumerate(versions[:3]):\n",
    "                    epochs_str = f\"[{', '.join(map(str, version['epochs'][:5]))}{'...' if len(version['epochs']) > 5 else ''}]\"\n",
    "                    print(f\"  {i+1}. {version['name']} - epochs: {epochs_str}\")\n",
    "                if len(versions) > 3:\n",
    "                    print(f\"  ... and {len(versions) - 3} more runs\")\n",
    "        else:\n",
    "            run_name_dropdown.options = ['No runs found']\n",
    "            run_name_dropdown.value = 'No runs found'\n",
    "            run_name_dropdown.disabled = True\n",
    "            epoch_dropdown.options = ['No epochs available']\n",
    "            epoch_dropdown.value = 'No epochs available'\n",
    "            epoch_dropdown.disabled = True\n",
    "            dataset_dropdown.options = ['No dataset available']\n",
    "            dataset_dropdown.value = 'No dataset available'\n",
    "            dataset_dropdown.disabled = True\n",
    "            \n",
    "            with output:\n",
    "                clear_output()\n",
    "                print(f\"‚ùå No runs found for {model_type}\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        with output:\n",
    "            clear_output()\n",
    "            print(f\"‚ùå Error loading runs for {model_type}: {e}\")\n",
    "        \n",
    "        run_name_dropdown.options = ['Error loading runs']\n",
    "        run_name_dropdown.value = 'Error loading runs'\n",
    "        run_name_dropdown.disabled = True\n",
    "        epoch_dropdown.options = ['Error loading epochs']\n",
    "        epoch_dropdown.value = 'Error loading epochs'\n",
    "        epoch_dropdown.disabled = True\n",
    "        dataset_dropdown.options = ['Error loading dataset']\n",
    "        dataset_dropdown.value = 'Error loading dataset'\n",
    "        dataset_dropdown.disabled = True\n",
    "\n",
    "# Function to update dataset when run name changes\n",
    "def update_dataset_for_run(change):\n",
    "    \"\"\"Update dataset dropdown based on selected run\"\"\"\n",
    "    model_type = model_dropdown.value\n",
    "    run_name = change['new'] if isinstance(change, dict) and 'new' in change else change\n",
    "    \n",
    "    try:\n",
    "        # Get dataset from wandb run\n",
    "        dataset_name = get_dataset_from_run(model_type, run_name)\n",
    "        \n",
    "        if dataset_name:\n",
    "            # Extract a readable name from the dataset path\n",
    "            dataset_display_name = dataset_name.split('/')[-1].replace('_', ' ').title()\n",
    "            \n",
    "            dataset_dropdown.options = [dataset_display_name]\n",
    "            dataset_dropdown.value = dataset_display_name\n",
    "            dataset_dropdown.disabled = True  # Keep it disabled since it's auto-detected\n",
    "            \n",
    "            # Store the actual dataset name for use in loading\n",
    "            DATASETS[dataset_display_name] = dataset_name\n",
    "            \n",
    "            with output:\n",
    "                current_text = output.outputs[-1]['text'] if output.outputs else \"\"\n",
    "                clear_output()\n",
    "                print(current_text)\n",
    "                print(f\"üìä Auto-detected dataset: {dataset_name}\")\n",
    "        else:\n",
    "            dataset_dropdown.options = ['Dataset not found in run']\n",
    "            dataset_dropdown.value = 'Dataset not found in run'\n",
    "            dataset_dropdown.disabled = True\n",
    "            \n",
    "            with output:\n",
    "                current_text = output.outputs[-1]['text'] if output.outputs else \"\"\n",
    "                clear_output()\n",
    "                print(current_text)\n",
    "                print(f\"‚ö†Ô∏è  Could not detect dataset for run {run_name}\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        dataset_dropdown.options = ['Error loading dataset']\n",
    "        dataset_dropdown.value = 'Error loading dataset'\n",
    "        dataset_dropdown.disabled = True\n",
    "        \n",
    "        with output:\n",
    "            current_text = output.outputs[-1]['text'] if output.outputs else \"\"\n",
    "            clear_output()\n",
    "            print(current_text)\n",
    "            print(f\"‚ùå Error loading dataset for {run_name}: {e}\")\n",
    "\n",
    "# Function to update available epochs when run name changes\n",
    "def update_epochs_for_run(change):\n",
    "    \"\"\"Update available epochs dropdown based on selected run\"\"\"\n",
    "    model_type = model_dropdown.value\n",
    "    run_name = change['new'] if isinstance(change, dict) and 'new' in change else change\n",
    "    \n",
    "    if model_type not in available_versions:\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        # Find the selected run in the versions\n",
    "        versions = available_versions[model_type]\n",
    "        selected_version = None\n",
    "        \n",
    "        for version in versions:\n",
    "            if version['name'] == run_name:\n",
    "                selected_version = version\n",
    "                break\n",
    "        \n",
    "        if selected_version and selected_version['epochs']:\n",
    "            epochs = [str(epoch) for epoch in sorted(selected_version['epochs'], reverse=True)]\n",
    "            epoch_dropdown.options = epochs\n",
    "            epoch_dropdown.value = epochs[0]  # Select the highest epoch by default\n",
    "            epoch_dropdown.disabled = False\n",
    "            \n",
    "            with output:\n",
    "                clear_output()\n",
    "                print(f\"‚úÖ Available epochs for {run_name}: {', '.join(epochs)}\")\n",
    "        else:\n",
    "            epoch_dropdown.options = ['No epochs available']\n",
    "            epoch_dropdown.value = 'No epochs available'\n",
    "            epoch_dropdown.disabled = True\n",
    "            \n",
    "            with output:\n",
    "                clear_output()\n",
    "                print(f\"‚ùå No epochs found for run {run_name}\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        with output:\n",
    "            clear_output()\n",
    "            print(f\"‚ùå Error loading epochs for {run_name}: {e}\")\n",
    "        \n",
    "        epoch_dropdown.options = ['Error loading epochs']\n",
    "        epoch_dropdown.value = 'Error loading epochs'\n",
    "        epoch_dropdown.disabled = True\n",
    "\n",
    "# Button to load configuration\n",
    "load_button = widgets.Button(\n",
    "    description='üöÄ Load Configuration',\n",
    "    button_style='primary',\n",
    "    layout=widgets.Layout(width='200px')\n",
    ")\n",
    "\n",
    "# Button to refresh available runs for current model\n",
    "refresh_runs_button = widgets.Button(\n",
    "    description='üîÑ Refresh Runs',\n",
    "    button_style='info',\n",
    "    layout=widgets.Layout(width='150px')\n",
    ")\n",
    "\n",
    "# Function to refresh runs for current model\n",
    "def refresh_current_model_runs(b=None):\n",
    "    \"\"\"Refresh runs for the currently selected model type\"\"\"\n",
    "    update_runs_for_model()\n",
    "\n",
    "# Function to load the selected configuration\n",
    "def load_configuration(b):\n",
    "    global current_pipeline, current_dataloader, current_config\n",
    "    \n",
    "    with output:\n",
    "        clear_output()\n",
    "        print(\"üîÑ Loading configuration...\")\n",
    "        \n",
    "        try:\n",
    "            # Get selected values\n",
    "            model_type = model_dropdown.value\n",
    "            dataset_name = DATASETS[dataset_dropdown.value]\n",
    "            run_name = run_name_dropdown.value\n",
    "            epoch = int(epoch_dropdown.value) if epoch_dropdown.value.isdigit() else 0\n",
    "            batch_size = batch_size_slider.value\n",
    "            image_size = image_size_dropdown.value\n",
    "            streaming = streaming_checkbox.value\n",
    "            \n",
    "            print(f\"üìã Configuration:\")\n",
    "            print(f\"  - Model: {model_type}\")\n",
    "            print(f\"  - Dataset: {dataset_dropdown.value} ({dataset_name})\")\n",
    "            print(f\"  - Run: {run_name}\")\n",
    "            print(f\"  - Epoch: {epoch}\")\n",
    "            print(f\"  - Batch size: {batch_size}\")\n",
    "            print(f\"  - Image size: {image_size}\")\n",
    "            print(f\"  - Streaming: {streaming}\")\n",
    "            \n",
    "            # Validate that we have a valid dataset\n",
    "            if dataset_name == 'Select a run first' or dataset_name == 'Dataset not found in run':\n",
    "                print(\"‚ùå No valid dataset detected. Please select a run with dataset information.\")\n",
    "                return\n",
    "            \n",
    "            # 1. Initialize pipeline\n",
    "            print(f\"\\nü§ñ Initializing {model_type} pipeline...\")\n",
    "            pipeline_class = MODEL_TYPES[model_type]\n",
    "            \n",
    "            scheduler_config_dict = get_scheduler_config_from_run(model_type, run_name)\n",
    "            if scheduler_config_dict is not None:\n",
    "                scheduler_config = BaseNextTokenPipeline.BaseSchedulerConfig()\n",
    "                scheduler_config.config = scheduler_config_dict\n",
    "                print(f\"‚úÖ Scheduler config loaded!\")\n",
    "            current_pipeline = pipeline_class(scheduler_config=scheduler_config)\n",
    "                \n",
    "            # 4. Create dataloader\n",
    "            print(f\"üóÇÔ∏è  Creating dataloader...\")\n",
    "            current_dataloader = ModularCharatersDataLoader.get_modular_char_dataloader(\n",
    "                dataset_name=dataset_name,\n",
    "                split='train',\n",
    "                image_size=image_size,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=True,\n",
    "                streaming=streaming,\n",
    "                conversionRGBA=True\n",
    "            )\n",
    "            print(f\"‚úÖ Dataloader created successfully\")\n",
    "            \n",
    "            # 3. Load model from hub\n",
    "            if run_name:\n",
    "                print(f\"üì• Loading model from hub...\")\n",
    "                current_pipeline.load_model_from_hub(run=run_name, epoch=epoch)\n",
    "                current_pipeline.set_num_class_embeds(len(dataloader.vocab))\n",
    "                print(f\"‚úÖ Model loaded successfully\")\n",
    "            else:\n",
    "                print(\"‚ö†Ô∏è  No run name specified - using default model weights\")\n",
    "            \n",
    "            \n",
    "            # 5. Store configuration\n",
    "            current_config = {\n",
    "                'model_type': model_type,\n",
    "                'dataset_name': dataset_name,\n",
    "                'run_name': run_name,\n",
    "                'epoch': epoch,\n",
    "                'batch_size': batch_size,\n",
    "                'image_size': image_size,\n",
    "                'streaming': streaming\n",
    "            }\n",
    "            \n",
    "            print(f\"\\nüéâ Configuration loaded successfully!\")\n",
    "            print(f\"‚úÖ Pipeline: {type(current_pipeline).__name__}\")\n",
    "            print(f\"‚úÖ Dataloader: Ready with {len(dataloader.vocab)} classes\")\n",
    "            print(f\"\\nüí° You can now use 'current_pipeline' and 'current_dataloader' for inference!\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading configuration: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "\n",
    "# Attach event handlers\n",
    "load_button.on_click(load_configuration)\n",
    "refresh_runs_button.on_click(refresh_current_model_runs)\n",
    "\n",
    "# Attach observers for dropdown changes\n",
    "model_dropdown.observe(update_runs_for_model, names='value')\n",
    "run_name_dropdown.observe(update_epochs_for_run, names='value')\n",
    "run_name_dropdown.observe(update_dataset_for_run, names='value')\n",
    "\n",
    "# Create the interface\n",
    "print(\"üéõÔ∏è Model Configuration Widget\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Display all widgets\n",
    "display(widgets.VBox([\n",
    "    widgets.HTML(\"<h3>üéõÔ∏è Model Configuration</h3>\"),\n",
    "    model_dropdown,\n",
    "    dataset_dropdown,\n",
    "    run_name_dropdown,\n",
    "    epoch_dropdown,\n",
    "    widgets.HTML(\"<h4>üìä Dataset Parameters</h4>\"),\n",
    "    batch_size_slider,\n",
    "    image_size_dropdown,\n",
    "    streaming_checkbox,\n",
    "    widgets.HTML(\"<h4>üîß Actions</h4>\"),\n",
    "    widgets.HBox([load_button, refresh_runs_button]),\n",
    "    output\n",
    "]))\n",
    "\n",
    "# Auto-load available runs for the default model on startup\n",
    "update_runs_for_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05932122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Inference with Selected Configuration\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.utils import make_grid\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def test_inference(num_samples=1, num_inference_steps=50):\n",
    "    \"\"\"Test inference with the currently loaded configuration\"\"\"\n",
    "    \n",
    "    if current_pipeline is None:\n",
    "        print(\"‚ùå No pipeline loaded! Please load a configuration first.\")\n",
    "        return\n",
    "    \n",
    "    if current_dataloader is None:\n",
    "        print(\"‚ùå No dataloader available! Please load a configuration first.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"üß™ Testing inference with current configuration...\")\n",
    "    print(f\"üìã Config: {current_config['model_type']}\")\n",
    "    print(f\"üóÇÔ∏è  Dataset: {current_config['dataset_name']}\")\n",
    "    print(f\"üéØ Run: {current_config['run_name']} (epoch {current_config['epoch']})\")\n",
    "    print(f\"üî¢ Inference steps: {num_inference_steps}\")\n",
    "    \n",
    "    try:\n",
    "        # Get a batch from the dataloader\n",
    "        sample_count = 0\n",
    "        for batch in current_dataloader:\n",
    "            if sample_count >= num_samples:\n",
    "                break\n",
    "                \n",
    "            input_images = batch['input'][:num_samples]\n",
    "            target_images = batch['target'][:num_samples]\n",
    "            labels = batch['label'][:num_samples]\n",
    "            \n",
    "            print(f\"\\nüé® Generating {input_images.shape[0]} image(s)...\")\n",
    "            print(f\"üìè Input shape: {input_images.shape}\")\n",
    "            print(f\"üè∑Ô∏è  Label shape: {labels.shape}\")\n",
    "            \n",
    "            # Run inference\n",
    "            with torch.no_grad():\n",
    "                outputs = current_pipeline(\n",
    "                    input_images=input_images, \n",
    "                    class_labels=labels,\n",
    "                    num_inference_steps=num_inference_steps\n",
    "                )\n",
    "            \n",
    "            print(f\"‚úÖ Generated output shape: {outputs.shape}\")\n",
    "            \n",
    "            # Display results\n",
    "            show_inference_results(input_images, outputs, target_images, batch.get('prompt', ['Unknown'] * len(input_images)))\n",
    "            \n",
    "            sample_count += input_images.shape[0]\n",
    "            \n",
    "        print(f\"\\nüéâ Inference completed successfully!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error during inference: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "def show_inference_results(input_images, output_images, target_images, prompts=None):\n",
    "    \"\"\"Display inference results in a grid\"\"\"\n",
    "    \n",
    "    # Denormalize images (from [-1, 1] to [0, 1])\n",
    "    def denormalize(tensor):\n",
    "        return (tensor * 0.5 + 0.5).clamp(0, 1).cpu()\n",
    "    \n",
    "    input_norm = denormalize(input_images)\n",
    "    output_norm = denormalize(output_images)\n",
    "    target_norm = denormalize(target_images)\n",
    "    \n",
    "    # Create comparison grid\n",
    "    batch_size = input_images.shape[0]\n",
    "    \n",
    "    # Concatenate all images: [input1, output1, target1, input2, output2, target2, ...]\n",
    "    all_images = []\n",
    "    for i in range(batch_size):\n",
    "        all_images.extend([input_norm[i], output_norm[i], target_norm[i]])\n",
    "    \n",
    "    # Stack into tensor\n",
    "    grid_tensor = torch.stack(all_images)\n",
    "    \n",
    "    # Create grid with 3 columns (input, output, target)\n",
    "    grid = make_grid(grid_tensor, nrow=3, padding=2, pad_value=1.0)\n",
    "    \n",
    "    # Convert to PIL and display\n",
    "    img = torchvision.transforms.ToPILImage()(grid)\n",
    "    \n",
    "    # Create figure with labels\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(12, 4 * batch_size))\n",
    "    ax.imshow(img)\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # Add column headers\n",
    "    ax.text(img.width * 0.17, -20, 'Input', ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "    ax.text(img.width * 0.50, -20, 'Generated', ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "    ax.text(img.width * 0.83, -20, 'Target', ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    # Add prompts if available\n",
    "    if prompts:\n",
    "        for i, prompt in enumerate(prompts):\n",
    "            y_pos = (i + 0.5) * (img.height / batch_size)\n",
    "            ax.text(-50, y_pos, f\"'{prompt}'\", ha='right', va='center', fontsize=10, \n",
    "                   rotation=90, fontweight='bold')\n",
    "    \n",
    "    plt.title(f\"Inference Results - {current_config.get('model_type', 'Unknown Model')}\", \n",
    "              fontsize=14, fontweight='bold', pad=30)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Interactive controls for inference testing\n",
    "inference_steps_slider = widgets.IntSlider(\n",
    "    value=50,\n",
    "    min=1,\n",
    "    max=100,\n",
    "    step=1,\n",
    "    description='Inference Steps:',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout=widgets.Layout(width='300px')\n",
    ")\n",
    "\n",
    "num_samples_slider = widgets.IntSlider(\n",
    "    value=1,\n",
    "    min=1,\n",
    "    max=8,\n",
    "    step=1,\n",
    "    description='Num Samples:',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout=widgets.Layout(width='300px')\n",
    ")\n",
    "\n",
    "test_button = widgets.Button(\n",
    "    description='üé® Test Inference',\n",
    "    button_style='success',\n",
    "    layout=widgets.Layout(width='150px')\n",
    ")\n",
    "\n",
    "def on_test_click(b):\n",
    "    test_inference(\n",
    "        num_samples=num_samples_slider.value,\n",
    "        num_inference_steps=inference_steps_slider.value\n",
    "    )\n",
    "\n",
    "test_button.on_click(on_test_click)\n",
    "\n",
    "# Display inference controls\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üé® Inference Testing\")\n",
    "display(widgets.VBox([\n",
    "    widgets.HTML(\"<h3>üé® Test Inference</h3>\"),\n",
    "    inference_steps_slider,\n",
    "    num_samples_slider,\n",
    "    test_button\n",
    "]))\n",
    "\n",
    "# Quick test function\n",
    "def quick_test():\n",
    "    \"\"\"Quick inference test with default parameters\"\"\"\n",
    "    test_inference(num_samples=2, num_inference_steps=20)\n",
    "\n",
    "print(\"\\nüí° Usage:\")\n",
    "print(\"1. Configure your model using the widget above\")\n",
    "print(\"2. Click 'üöÄ Load Configuration' to initialize\")\n",
    "print(\"3. Use the inference controls or call quick_test() for a fast test\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
