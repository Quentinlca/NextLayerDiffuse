{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6c90341",
   "metadata": {},
   "source": [
    "TODO:\n",
    "- Choose another architecture for the dataset Maybe without <BOS> and <EOS>\n",
    "- Include the prompts as a guidance maybe in the decoding process add a channel with the prompt for the next layer (Add a \"end\" prompt to indicate that the generation is over ?)\n",
    "- Transfer to github\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7deed058",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "\n",
    "# Import custom modules\n",
    "from models.AutoregressiveCNNGenerator import AutoregressiveCNNGenerator\n",
    "from loss.RGBAMSELoss import RGBAWeightedProportionalMSELoss\n",
    "\n",
    "# Import functions\n",
    "from data_loaders.data_loaders import get_loaders\n",
    "from utils.utils_functions import *\n",
    "\n",
    "# Import constants\n",
    "from constants import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e8d59f",
   "metadata": {},
   "source": [
    "### Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74760a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(dataset_path, output_dir, checkpoint_path=None, batch_size=32, num_epochs=50, lr=0.001):\n",
    "    \"\"\"\n",
    "    Train the autoregressive RGBA character generator model with pre-processed PyTorch data.\n",
    "    \n",
    "    Args:\n",
    "        dataset_path: Path to the PyTorch dataset file\n",
    "        output_dir: Directory to save outputs\n",
    "        batch_size: Batch size for training\n",
    "        num_epochs: Number of epochs to train\n",
    "        lr: Learning rate\n",
    "    \"\"\"\n",
    "    # Create output directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Set device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Get dataloaders with fast loading\n",
    "    print(\"Loading dataset...\")\n",
    "    data_loading_start = time.time()\n",
    "    train_loader, val_loader = get_loaders(\n",
    "        dataset_path,\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "    data_loading_time = time.time() - data_loading_start\n",
    "    print(f\"Dataset loaded in {data_loading_time:.2f}s\")\n",
    "    print(f\"Train batches: {len(train_loader)}, Val batches: {len(val_loader)}\")\n",
    "    \n",
    "    # Initialize model\n",
    "    model = AutoregressiveCNNGenerator().to(device)\n",
    "    if checkpoint_path:\n",
    "        # Load model from checkpoint if provided\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        print(f\"Model loaded from epoch {checkpoint['epoch']}, Train Loss: {checkpoint['train_loss']:.6f}, Val Loss: {checkpoint['val_loss']:.6f}\")\n",
    "    \n",
    "    # Loss function and optimizer\n",
    "    criterion = RGBAWeightedProportionalMSELoss(threshold=0.05, alpha_channel_weight=2.0)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    # Creating WandB run\n",
    "    print(\"Creating WandB run...\")\n",
    "    run = wandb.init(entity=\"quentinlca-perso\",  # Replace with your WandB entity\n",
    "                     project=\"DTU-internship\",\n",
    "                     name=\"train_autoregressive_rgba_generator\",\n",
    "                     config={\n",
    "                        \"model_name\": model.__class__.__name__,\n",
    "                        \"loss_function\": {\n",
    "                            'name': criterion.__class__.__name__,\n",
    "                            'params': list(criterion.parameters()),\n",
    "                        },\n",
    "                        \"optimizer\": optimizer.__class__.__name__,\n",
    "                        \"device\": str(device),\n",
    "                        \"checkpoint_path\": checkpoint_path if checkpoint_path else \"None\",\n",
    "                        \"dataset\": os.path.basename(dataset_path),\n",
    "                        \"dataset_size\": len(train_loader.dataset) + len(val_loader.dataset),\n",
    "                        \"batch_size\": batch_size,\n",
    "                        \"num_epochs\": num_epochs,\n",
    "                        \"learning_rate\": lr\n",
    "                     },\n",
    "                     resume='allow' if checkpoint_path else None,\n",
    "                     id=checkpoint['run_id'] if checkpoint_path else None)\n",
    "    \n",
    "    # Training loop\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    total_train_time = 0\n",
    "    \n",
    "    total_epoch = checkpoint['epoch'] if checkpoint_path else 0\n",
    "    last_epoch = total_epoch + num_epochs\n",
    "    print(\"Resuming training from epoch:\", total_epoch + 1)\n",
    "    for epoch in range(num_epochs):\n",
    "        total_epoch += 1\n",
    "        epoch_start = time.time()\n",
    "        print(f\"\\nEpoch {total_epoch}/{last_epoch}\")\n",
    "        \n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        for inputs, targets in tqdm(train_loader, desc=\"Training\"):\n",
    "            # Move data to device\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            # Zero gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = criterion(inputs, outputs, targets)\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # Average training loss\n",
    "        train_loss /= len(train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in tqdm(val_loader, desc=\"Validating\"):\n",
    "                # Move data to device\n",
    "                inputs = inputs.to(device)\n",
    "                targets = targets.to(device)\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = model(inputs)  # No layer_idx needed for validation\n",
    "                \n",
    "                # Calculate loss\n",
    "                loss = criterion(inputs, outputs, targets)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                # Save the first batch for visualization\n",
    "                if epoch % 5 == 0 and val_loss == loss.item():\n",
    "                    example_inputs = inputs\n",
    "                    example_targets = targets\n",
    "                    example_outputs = outputs\n",
    "        \n",
    "        # Average validation loss\n",
    "        val_loss /= len(val_loader)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        epoch_time = time.time() - epoch_start\n",
    "        total_train_time += epoch_time\n",
    "        \n",
    "        print(f\"Epoch {total_epoch} - Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}\")\n",
    "        print(f\"Epoch time: {epoch_time:.2f}s\")\n",
    "        \n",
    "        # Save model checkpoint\n",
    "        if (epoch) % 10 == 0 or epoch == num_epochs - 1:\n",
    "            print(f\"Saving checkpoint for epoch {total_epoch}...\")\n",
    "            torch.save({\n",
    "                'epoch': total_epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'train_loss': train_loss,\n",
    "                'val_loss': val_loss,\n",
    "                'run_id': run.id\n",
    "            }, os.path.join(output_dir, f'checkpoint_epoch_{total_epoch}.pt'))\n",
    "        \n",
    "        # Visualize results every 5 epochs\n",
    "        if epoch % 5 == 0 or epoch == num_epochs - 1:\n",
    "            print(\"Saving example images...\")\n",
    "            save_examples(example_inputs, \n",
    "                          example_targets, \n",
    "                          example_outputs, \n",
    "                          f\"{output_dir}/results_epoch_{total_epoch}\")\n",
    "        # Log to WandB\n",
    "        print(\"Logging to WandB...\")\n",
    "        run.log({\n",
    "            \"epoch\": total_epoch,\n",
    "            \"train_loss\": train_loss,\n",
    "            \"val_loss\": val_loss,\n",
    "            \"epoch_time\": epoch_time,\n",
    "            \"total_train_time\": total_train_time\n",
    "            \n",
    "        })\n",
    "    \n",
    "    # Plot loss curves\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(output_dir, 'loss_curve.png'))\n",
    "    \n",
    "    # Generate and save a full sequence\n",
    "    # generate_full_rgba_sequence(model, device, os.path.join(output_dir, 'full_sequence.png'))\n",
    "    \n",
    "    avg_epoch_time = total_train_time / num_epochs\n",
    "    print(f\"Training completed. Results saved to {output_dir}\")\n",
    "    print(f\"Total training time: {total_train_time:.2f}s, Average epoch time: {avg_epoch_time:.2f}s\")\n",
    "    \n",
    "    run.finish()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65150860",
   "metadata": {},
   "source": [
    "### Main Cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efff95aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train_model(\n",
    "    dataset_path='datasets/LIAON-MuLAn_v1.pt',  # Replace with your dataset path\n",
    "    output_dir='training_output',  # Replace with your desired output directory\n",
    "    checkpoint_path='training_output/checkpoint_epoch_10.pt',  # Optional: Load from a checkpoint\n",
    "    batch_size=32,\n",
    "    num_epochs=10,\n",
    "    lr=0.005\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
