{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2cd5922a",
   "metadata": {},
   "source": [
    "# DDIM Next Token V1 Training Debugging Notebook\n",
    "\n",
    "This notebook is designed to reproduce and debug the NaN/Inf gradient and AMP unscale_() errors encountered during training.\n",
    "\n",
    "## Error Details\n",
    "- **Warning**: NaN or Inf gradient norm detected at step 106, epoch 0\n",
    "- **RuntimeError**: unscale_() has already been called on this optimizer since the last update()\n",
    "\n",
    "This error occurs when the automatic mixed precision (AMP) gradient scaler tries to unscale gradients multiple times without an optimizer step in between.\n",
    "\n",
    "## Training Context\n",
    "- Model: DDIMNextTokenV1\n",
    "- Learning Rate: 2.12e-5  \n",
    "- Step: 105-106\n",
    "- Loss: 0.63\n",
    "\n",
    "Let's systematically debug and fix these issues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652d04ca",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "Import torch, accelerate, and all necessary modules for model, data loading, and training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56856a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core PyTorch imports\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "# Accelerate and diffusers\n",
    "from accelerate import Accelerator, notebook_launcher\n",
    "from diffusers.optimization import get_cosine_schedule_with_warmup\n",
    "\n",
    "# Model and data loading imports\n",
    "from models import DDIMNextTokenV1\n",
    "from data_loaders import ModularCharatersDataLoader\n",
    "\n",
    "# Utilities\n",
    "import os\n",
    "import time\n",
    "import wandb\n",
    "from tqdm.auto import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "# For debugging\n",
    "import traceback\n",
    "import warnings\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"CUDA memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295e5f21",
   "metadata": {},
   "source": [
    "## 2. Set Training Hyperparameters\n",
    "\n",
    "Define hyperparameters matching the error context where the issue occurred.\n",
    "Using conservative settings to minimize NaN occurrence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def98025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training hyperparameters based on the error context\n",
    "model_version = \"DDIMNextTokenV1\"\n",
    "dataset_name = \"QLeca/modular_characters_hairs_RGB\"\n",
    "\n",
    "# Core training parameters\n",
    "train_size = 1000  # Reduced for debugging\n",
    "val_size = 100     # Reduced for debugging  \n",
    "batch_size = 8     # Reduced from 16 to help stability\n",
    "num_epochs = 3     # Reduced for debugging\n",
    "\n",
    "# Learning rate and scheduler parameters\n",
    "learning_rate = 1e-6  # Very conservative (original error had 2.12e-5)\n",
    "warming_steps = 100   # Reduced warmup\n",
    "num_cycles = 0.5\n",
    "\n",
    "# Debugging parameters\n",
    "max_nan_tolerance = 5  # Lower tolerance for debugging\n",
    "debug_mode = True      # Enable detailed logging\n",
    "\n",
    "# Model parameters\n",
    "mixed_precision = \"no\"  # Disable AMP to avoid unscale_() issues initially\n",
    "gradient_clip_value = 0.5  # Conservative gradient clipping\n",
    "\n",
    "print(\"Hyperparameters set:\")\n",
    "print(f\"  Model: {model_version}\")\n",
    "print(f\"  Dataset: {dataset_name}\")\n",
    "print(f\"  Learning Rate: {learning_rate}\")\n",
    "print(f\"  Batch Size: {batch_size}\")\n",
    "print(f\"  Mixed Precision: {mixed_precision}\")\n",
    "print(f\"  Gradient Clipping: {gradient_clip_value}\")\n",
    "print(f\"  Train/Val Size: {train_size}/{val_size}\")\n",
    "\n",
    "# Additional training tags for experiment tracking\n",
    "train_tags = [\"debugging\", \"nan_fix\", \"conservative_settings\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479e0405",
   "metadata": {},
   "source": [
    "## 3. Initialize Model Pipeline\n",
    "\n",
    "Instantiate the DDIMNextTokenV1Pipeline and configure its training parameters.\n",
    "We'll override the default settings to be more conservative for stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d64dd55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the pipeline\n",
    "print(\"Initializing DDIMNextTokenV1 Pipeline...\")\n",
    "pipeline = DDIMNextTokenV1.DDIMNextTokenV1Pipeline()\n",
    "\n",
    "# Configure training parameters for stability\n",
    "pipeline.train_config.train_batch_size = batch_size\n",
    "pipeline.train_config.eval_batch_size = batch_size\n",
    "pipeline.train_config.num_epochs = num_epochs\n",
    "pipeline.train_config.learning_rate = learning_rate\n",
    "pipeline.train_config.lr_warmup_steps = warming_steps\n",
    "pipeline.train_config.mixed_precision = mixed_precision\n",
    "\n",
    "# Additional safety configurations\n",
    "pipeline.train_config.save_image_epochs = 1  # Save images every epoch for debugging\n",
    "pipeline.train_config.save_model_epochs = 1  # Save model every epoch for debugging\n",
    "\n",
    "print(\"Pipeline initialized successfully!\")\n",
    "print(f\"  Device: {pipeline.device}\")\n",
    "print(f\"  Image Size: {pipeline.train_config.image_size}\")\n",
    "print(f\"  Model Config: {pipeline.model_config.config['in_channels']} → {pipeline.model_config.config['out_channels']} channels\")\n",
    "\n",
    "# Check initial model state\n",
    "print(\"\\nInitial model parameter check:\")\n",
    "nan_params = pipeline.check_model_for_nan()\n",
    "if nan_params:\n",
    "    print(\"⚠️  WARNING: Model already contains NaN parameters!\")\n",
    "else:\n",
    "    print(\"✅ Model parameters are clean (no NaN values)\")\n",
    "\n",
    "# Print model summary\n",
    "total_params = sum(p.numel() for p in pipeline.unet.parameters())\n",
    "trainable_params = sum(p.numel() for p in pipeline.unet.parameters() if p.requires_grad)\n",
    "print(f\"\\nModel Summary:\")\n",
    "print(f\"  Total parameters: {total_params:,}\")\n",
    "print(f\"  Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3b9146",
   "metadata": {},
   "source": [
    "## 4. Prepare Data Loaders\n",
    "\n",
    "Set up the training and validation data loaders using the ModularCharatersDataLoader.\n",
    "We'll also validate the data to ensure it doesn't contain NaN values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f428e066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders\n",
    "print(\"Setting up data loaders...\")\n",
    "try:\n",
    "    train_dataloader = ModularCharatersDataLoader.get_modular_char_dataloader(\n",
    "        dataset_name=dataset_name,\n",
    "        split=\"train\",\n",
    "        image_size=pipeline.train_config.image_size,\n",
    "        batch_size=pipeline.train_config.train_batch_size,\n",
    "        shuffle=True,\n",
    "    )\n",
    "    \n",
    "    val_dataloader = ModularCharatersDataLoader.get_modular_char_dataloader(\n",
    "        dataset_name=dataset_name,\n",
    "        split=\"train\",  # Using same split for validation during debugging\n",
    "        image_size=pipeline.train_config.image_size,\n",
    "        batch_size=pipeline.train_config.eval_batch_size,\n",
    "        shuffle=True,\n",
    "    )\n",
    "    \n",
    "    print(\"✅ Data loaders created successfully!\")\n",
    "    print(f\"  Train dataset size: {len(train_dataloader.dataset)}\")\n",
    "    print(f\"  Val dataset size: {len(val_dataloader.dataset)}\")\n",
    "    print(f\"  Train batches: {len(train_dataloader)}\")\n",
    "    print(f\"  Val batches: {len(val_dataloader)}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error creating data loaders: {e}\")\n",
    "    traceback.print_exc()\n",
    "    raise\n",
    "\n",
    "# Validate a sample batch for NaN values\n",
    "print(\"\\nValidating sample batch for NaN values...\")\n",
    "try:\n",
    "    sample_batch = next(iter(train_dataloader))\n",
    "    input_images = sample_batch[\"input\"]\n",
    "    target_images = sample_batch[\"target\"]\n",
    "    class_labels = sample_batch[\"label\"]\n",
    "    \n",
    "    print(f\"  Input shape: {input_images.shape}\")\n",
    "    print(f\"  Target shape: {target_images.shape}\")\n",
    "    print(f\"  Labels shape: {class_labels.shape}\")\n",
    "    \n",
    "    # Check for NaN values in the data\n",
    "    input_nan = torch.isnan(input_images).any()\n",
    "    target_nan = torch.isnan(target_images).any()\n",
    "    \n",
    "    if input_nan or target_nan:\n",
    "        print(f\"❌ WARNING: NaN values found in data!\")\n",
    "        print(f\"  Input NaN: {input_nan}\")\n",
    "        print(f\"  Target NaN: {target_nan}\")\n",
    "    else:\n",
    "        print(\"✅ Sample batch data is clean (no NaN values)\")\n",
    "        \n",
    "    # Print data statistics\n",
    "    print(f\"  Input range: [{input_images.min():.3f}, {input_images.max():.3f}]\")\n",
    "    print(f\"  Target range: [{target_images.min():.3f}, {target_images.max():.3f}]\")\n",
    "    print(f\"  Unique labels: {torch.unique(class_labels).tolist()}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error validating batch: {e}\")\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40c8e75",
   "metadata": {},
   "source": [
    "## 5. Custom Training Loop with Gradient Norm Handling\n",
    "\n",
    "Implement a training loop that includes proper gradient norm clipping and checks for NaN/Inf gradients.\n",
    "This version will handle the AMP scaler state correctly to avoid the unscale_() error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8480ad31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_training_step(pipeline, batch, optimizer, lr_scheduler, accelerator, step, epoch, \n",
    "                       nan_count, max_nan_tolerance, gradient_clip_value):\n",
    "    \"\"\"\n",
    "    Perform a single training step with comprehensive NaN/Inf handling.\n",
    "    Returns (loss_value, nan_count, should_continue)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract batch data\n",
    "    input_images = batch[\"input\"].to(pipeline.device)\n",
    "    target_images = batch[\"target\"].to(pipeline.device)\n",
    "    class_labels = batch['label'].to(pipeline.device)\n",
    "    \n",
    "    # Check for NaN values in input data\n",
    "    if torch.isnan(input_images).any() or torch.isnan(target_images).any():\n",
    "        print(f\"❌ Step {step}: NaN in input data, skipping batch\")\n",
    "        return None, nan_count + 1, nan_count + 1 <= max_nan_tolerance\n",
    "    \n",
    "    # Sample noise and timesteps\n",
    "    noise = torch.randn(target_images.shape, device=pipeline.device)\n",
    "    bs = target_images.shape[0]\n",
    "    timesteps = torch.randint(\n",
    "        0, pipeline.scheduler.config['num_train_timesteps'], (bs,), \n",
    "        device=pipeline.device, dtype=torch.int\n",
    "    )\n",
    "    \n",
    "    # Add noise to clean images\n",
    "    noisy_targets = pipeline.scheduler.add_noise(target_images, noise, timesteps)\n",
    "    \n",
    "    # Forward pass\n",
    "    with accelerator.accumulate(pipeline.unet):\n",
    "        noisy_samples = torch.concat([input_images, noisy_targets], dim=1)\n",
    "        noise_pred = pipeline.unet.forward(\n",
    "            sample=noisy_samples,\n",
    "            timestep=timesteps,\n",
    "            class_labels=class_labels\n",
    "        ).sample\n",
    "        \n",
    "        loss = F.mse_loss(noise_pred, noise)\n",
    "        \n",
    "        # Check for NaN/Inf loss\n",
    "        if torch.isnan(loss) or torch.isinf(loss):\n",
    "            print(f\"❌ Step {step}: NaN/Inf loss ({loss.item()}), skipping batch\")\n",
    "            return None, nan_count + 1, nan_count + 1 <= max_nan_tolerance\n",
    "        \n",
    "        # Check for NaN in noise prediction\n",
    "        if torch.isnan(noise_pred).any():\n",
    "            print(f\"❌ Step {step}: NaN in noise prediction, skipping batch\")\n",
    "            return None, nan_count + 1, nan_count + 1 <= max_nan_tolerance\n",
    "        \n",
    "        # Backward pass\n",
    "        accelerator.backward(loss)\n",
    "        \n",
    "        # Gradient handling with proper AMP scaler management\n",
    "        if accelerator.sync_gradients:\n",
    "            # Check for NaN gradients BEFORE any unscaling\n",
    "            has_nan_gradients = False\n",
    "            for name, param in pipeline.unet.named_parameters():\n",
    "                if param.grad is not None and (torch.isnan(param.grad).any() or torch.isinf(param.grad).any()):\n",
    "                    print(f\"❌ Step {step}: NaN/Inf gradient in {name}\")\n",
    "                    has_nan_gradients = True\n",
    "                    break\n",
    "            \n",
    "            if has_nan_gradients:\n",
    "                # Zero gradients but still step optimizer to maintain scaler state\n",
    "                optimizer.zero_grad()\n",
    "                optimizer.step()\n",
    "                lr_scheduler.step()\n",
    "                print(f\"⚠️  Step {step}: Zeroed gradients due to NaN, but stepped optimizer\")\n",
    "                return None, nan_count + 1, nan_count + 1 <= max_nan_tolerance\n",
    "            else:\n",
    "                # Normal gradient clipping and optimization\n",
    "                accelerator.clip_grad_norm_(pipeline.unet.parameters(), gradient_clip_value)\n",
    "        \n",
    "        # Normal optimizer step\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        return loss.detach().item(), nan_count, True\n",
    "\n",
    "# Prepare extra parameters for training\n",
    "extra_kwargs = {\n",
    "    \"num_cycles\": num_cycles,\n",
    "    \"train_tags\": train_tags,\n",
    "}\n",
    "\n",
    "print(\"Starting custom training with enhanced NaN handling...\")\n",
    "print(f\"Max NaN tolerance: {max_nan_tolerance}\")\n",
    "print(f\"Gradient clipping value: {gradient_clip_value}\")\n",
    "print(f\"Mixed precision: {mixed_precision}\")\n",
    "\n",
    "# Start training with the pipeline's built-in method first\n",
    "try:\n",
    "    # Use the fixed pipeline training method\n",
    "    pipeline.train_accelerate(\n",
    "        train_dataloader=train_dataloader,\n",
    "        val_dataloader=val_dataloader,\n",
    "        train_size=train_size,\n",
    "        val_size=val_size,\n",
    "        **extra_kwargs\n",
    "    )\n",
    "    print(\"✅ Training completed successfully!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Training failed with error: {e}\")\n",
    "    traceback.print_exc()\n",
    "    \n",
    "    # If the main training fails, we'll implement a custom training loop below\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"MAIN TRAINING FAILED - Implementing custom debug training loop...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28932e3d",
   "metadata": {},
   "source": [
    "## 6. Debugging NaN/Inf Gradients and AMP Issues\n",
    "\n",
    "Add comprehensive debugging tools to catch and analyze AMP unscale_() errors and NaN gradient issues.\n",
    "This section includes utilities to monitor optimizer and scaler states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc1f631",
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug_amp_scaler_state(accelerator):\n",
    "    \"\"\"Debug the AMP scaler state to understand unscale_() issues.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"AMP SCALER DEBUG INFO\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    if hasattr(accelerator, 'scaler') and accelerator.scaler is not None:\n",
    "        scaler = accelerator.scaler\n",
    "        print(f\"Scaler enabled: {scaler.is_enabled()}\")\n",
    "        print(f\"Scaler scale: {scaler.get_scale()}\")\n",
    "        print(f\"Growth tracker: {scaler.get_growth_tracker()}\")\n",
    "        \n",
    "        # Check if scaler state is consistent\n",
    "        for i, optimizer in enumerate(accelerator.optimizer if isinstance(accelerator.optimizer, list) else [accelerator.optimizer]):\n",
    "            print(f\"Optimizer {i}: {type(optimizer).__name__}\")\n",
    "            print(f\"  State dict keys: {list(optimizer.state_dict().keys())}\")\n",
    "            \n",
    "    else:\n",
    "        print(\"No AMP scaler found (expected for mixed_precision='no')\")\n",
    "    \n",
    "    print(\"=\"*50)\n",
    "\n",
    "def debug_gradient_state(model, step_info=\"\"):\n",
    "    \"\"\"Debug gradient state of model parameters.\"\"\"\n",
    "    print(f\"\\nGRADIENT DEBUG {step_info}\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    total_params = 0\n",
    "    params_with_grad = 0\n",
    "    nan_grads = 0\n",
    "    inf_grads = 0\n",
    "    zero_grads = 0\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        total_params += 1\n",
    "        if param.grad is not None:\n",
    "            params_with_grad += 1\n",
    "            if torch.isnan(param.grad).any():\n",
    "                nan_grads += 1\n",
    "                print(f\"❌ NaN gradient in: {name}\")\n",
    "            elif torch.isinf(param.grad).any():\n",
    "                inf_grads += 1\n",
    "                print(f\"❌ Inf gradient in: {name}\")\n",
    "            elif torch.allclose(param.grad, torch.zeros_like(param.grad)):\n",
    "                zero_grads += 1\n",
    "    \n",
    "    print(f\"Total parameters: {total_params}\")\n",
    "    print(f\"Parameters with gradients: {params_with_grad}\")\n",
    "    print(f\"Parameters with NaN gradients: {nan_grads}\")\n",
    "    print(f\"Parameters with Inf gradients: {inf_grads}\")\n",
    "    print(f\"Parameters with zero gradients: {zero_grads}\")\n",
    "    \n",
    "    return nan_grads > 0 or inf_grads > 0\n",
    "\n",
    "def custom_debug_training_loop():\n",
    "    \"\"\"\n",
    "    Custom training loop with extensive debugging for AMP and gradient issues.\n",
    "    This will help us understand exactly when and why the unscale_() error occurs.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"STARTING CUSTOM DEBUG TRAINING LOOP\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Reset pipeline to clean state\n",
    "    pipeline.train_id = f\"debug_run_{time.strftime('%Y-%m-%d_%H-%M-%S')}\"\n",
    "    pipeline.set_num_class_embeds(len(train_dataloader.vocab))\n",
    "    \n",
    "    # Create optimizer and scheduler manually for better control\n",
    "    optimizer = torch.optim.AdamW(pipeline.unet.parameters(), lr=learning_rate)\n",
    "    total_steps = (train_size * num_epochs) // batch_size\n",
    "    lr_scheduler = get_cosine_schedule_with_warmup(\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=warming_steps,\n",
    "        num_training_steps=total_steps,\n",
    "        num_cycles=num_cycles\n",
    "    )\n",
    "    \n",
    "    # Initialize accelerator with debug settings\n",
    "    accelerator = Accelerator(\n",
    "        mixed_precision=mixed_precision,\n",
    "        gradient_accumulation_steps=1,\n",
    "        log_with=\"tensorboard\",\n",
    "        project_dir=os.path.join(pipeline.train_config.output_dir, \"debug_logs\")\n",
    "    )\n",
    "    \n",
    "    # Prepare everything\n",
    "    pipeline.unet, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n",
    "        pipeline.unet, optimizer, train_dataloader, lr_scheduler\n",
    "    )\n",
    "    \n",
    "    print(f\"Accelerator state:\")\n",
    "    print(f\"  Mixed precision: {accelerator.mixed_precision}\")\n",
    "    print(f\"  Use distributed: {accelerator.use_distributed}\")\n",
    "    print(f\"  Device: {accelerator.device}\")\n",
    "    \n",
    "    # Debug initial state\n",
    "    debug_amp_scaler_state(accelerator)\n",
    "    debug_gradient_state(pipeline.unet, \"INITIAL\")\n",
    "    \n",
    "    global_step = 0\n",
    "    nan_count = 0\n",
    "    \n",
    "    # Training loop with extensive debugging\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\n{'='*40}\")\n",
    "        print(f\"EPOCH {epoch} - Debug Training\")\n",
    "        print(f\"{'='*40}\")\n",
    "        \n",
    "        pipeline.unet.train()\n",
    "        \n",
    "        # Take a small subset for debugging\n",
    "        debug_batches = min(10, len(train_dataloader))  # Only first 10 batches for debugging\n",
    "        \n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            if step >= debug_batches:\n",
    "                break\n",
    "                \n",
    "            print(f\"\\n--- Step {step} (Global: {global_step}) ---\")\n",
    "            \n",
    "            try:\n",
    "                # Manual training step with debugging\n",
    "                input_images = batch[\"input\"].to(pipeline.device)\n",
    "                target_images = batch[\"target\"].to(pipeline.device)\n",
    "                class_labels = batch['label'].to(pipeline.device)\n",
    "                \n",
    "                # Pre-step debugging\n",
    "                debug_amp_scaler_state(accelerator)\n",
    "                \n",
    "                # Check input data\n",
    "                if torch.isnan(input_images).any() or torch.isnan(target_images).any():\n",
    "                    print(f\"❌ NaN in input data at step {step}\")\n",
    "                    nan_count += 1\n",
    "                    continue\n",
    "                \n",
    "                # Sample noise and forward pass\n",
    "                noise = torch.randn(target_images.shape, device=pipeline.device)\n",
    "                bs = target_images.shape[0]\n",
    "                timesteps = torch.randint(0, pipeline.scheduler.config['num_train_timesteps'], \n",
    "                                        (bs,), device=pipeline.device, dtype=torch.int)\n",
    "                \n",
    "                noisy_targets = pipeline.scheduler.add_noise(target_images, noise, timesteps)\n",
    "                \n",
    "                with accelerator.accumulate(pipeline.unet):\n",
    "                    noisy_samples = torch.concat([input_images, noisy_targets], dim=1)\n",
    "                    noise_pred = pipeline.unet.forward(sample=noisy_samples, timestep=timesteps, \n",
    "                                                     class_labels=class_labels).sample\n",
    "                    loss = F.mse_loss(noise_pred, noise)\n",
    "                    \n",
    "                    print(f\"Loss: {loss.item():.6f}\")\n",
    "                    \n",
    "                    if torch.isnan(loss) or torch.isinf(loss):\n",
    "                        print(f\"❌ NaN/Inf loss: {loss.item()}\")\n",
    "                        nan_count += 1\n",
    "                        if nan_count > max_nan_tolerance:\n",
    "                            print(\"Too many NaN occurrences, stopping\")\n",
    "                            return\n",
    "                        continue\n",
    "                    \n",
    "                    # Backward pass\n",
    "                    accelerator.backward(loss)\n",
    "                    \n",
    "                    # Post-backward debugging\n",
    "                    print(\"After backward pass:\")\n",
    "                    has_bad_grads = debug_gradient_state(pipeline.unet, f\"STEP_{step}_POST_BACKWARD\")\n",
    "                    \n",
    "                    if accelerator.sync_gradients:\n",
    "                        print(\"Syncing gradients...\")\n",
    "                        debug_amp_scaler_state(accelerator)\n",
    "                        \n",
    "                        if has_bad_grads:\n",
    "                            print(\"❌ Bad gradients detected, zeroing and stepping anyway\")\n",
    "                            optimizer.zero_grad()\n",
    "                        else:\n",
    "                            print(\"✅ Gradients look good, clipping...\")\n",
    "                            try:\n",
    "                                grad_norm = accelerator.clip_grad_norm_(pipeline.unet.parameters(), gradient_clip_value)\n",
    "                                print(f\"Gradient norm after clipping: {grad_norm}\")\n",
    "                            except Exception as e:\n",
    "                                print(f\"❌ Error during gradient clipping: {e}\")\n",
    "                                traceback.print_exc()\n",
    "                                nan_count += 1\n",
    "                                if nan_count > max_nan_tolerance:\n",
    "                                    return\n",
    "                                optimizer.zero_grad()\n",
    "                    \n",
    "                    # Optimizer step\n",
    "                    print(\"Stepping optimizer...\")\n",
    "                    try:\n",
    "                        optimizer.step()\n",
    "                        lr_scheduler.step()\n",
    "                        optimizer.zero_grad()\n",
    "                        print(\"✅ Optimizer step successful\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"❌ Error during optimizer step: {e}\")\n",
    "                        traceback.print_exc()\n",
    "                        nan_count += 1\n",
    "                        if nan_count > max_nan_tolerance:\n",
    "                            return\n",
    "                        optimizer.zero_grad()\n",
    "                \n",
    "                global_step += 1\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"❌ Error in training step {step}: {e}\")\n",
    "                traceback.print_exc()\n",
    "                nan_count += 1\n",
    "                if nan_count > max_nan_tolerance:\n",
    "                    print(\"Too many errors, stopping debug training\")\n",
    "                    return\n",
    "        \n",
    "        print(f\"\\nCompleted epoch {epoch} debug training\")\n",
    "        print(f\"Total NaN/error count: {nan_count}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"DEBUG TRAINING LOOP COMPLETED\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "# Run the debug training loop if main training failed\n",
    "print(\"Running custom debug training loop...\")\n",
    "custom_debug_training_loop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce82637",
   "metadata": {},
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "This notebook provides comprehensive debugging for the NaN gradient and AMP unscale_() issues.\n",
    "\n",
    "### Key Debugging Features:\n",
    "1. **Conservative Training Settings** - Reduced learning rate, batch size, and disabled mixed precision\n",
    "2. **Comprehensive NaN Checking** - Input data, loss, gradients, and model parameters\n",
    "3. **AMP Scaler State Monitoring** - Debug the gradient scaler state to prevent unscale_() errors\n",
    "4. **Gradient State Analysis** - Detailed inspection of parameter gradients\n",
    "5. **Error Recovery** - Graceful handling of NaN/Inf values with early stopping\n",
    "\n",
    "### Fixes Applied to the Main Model:\n",
    "- Fixed the gradient checking logic to avoid the unscale_() error\n",
    "- Always call optimizer.step() even when gradients are zeroed to maintain scaler consistency\n",
    "- Added comprehensive NaN checking throughout the training pipeline\n",
    "- Implemented conservative training settings for stability\n",
    "\n",
    "### Recommendations:\n",
    "1. **Start with conservative settings** (as set in this notebook)\n",
    "2. **Monitor NaN count** - if it's consistently high, check your data preprocessing\n",
    "3. **Gradually increase learning rate** once training is stable\n",
    "4. **Re-enable mixed precision** only after confirming stability with float32\n",
    "5. **Check your dataset** for corrupted or extreme values that might cause NaN\n",
    "\n",
    "Run each cell in sequence to debug your training issues systematically."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
