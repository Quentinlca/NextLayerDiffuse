{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0aaccfe4",
   "metadata": {},
   "source": [
    "### Imports and constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983434d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import json\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "output_dir = 'kenney_modular_characters'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "assets_dir = 'assets/kenney_new'\n",
    "MODULES_ORDER = [\n",
    "    'Arm_L',\n",
    "    'Arm_R',\n",
    "    'Neck',\n",
    "    'Head',\n",
    "    'Hand_L',\n",
    "    'Hand_R',\n",
    "    'Shirt',\n",
    "    'Shirt_L',\n",
    "    'Shirt_R',\n",
    "    'Leg_L',\n",
    "    'Leg_R',\n",
    "    'Shoes_L',\n",
    "    'Shoes_R',\n",
    "    'Pants_L',\n",
    "    'Pants_R',\n",
    "    'Pants',\n",
    "    'Face',\n",
    "    'Hair'\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87b1311",
   "metadata": {},
   "source": [
    "### To update all the offsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4208a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "modules_offsets_file = 'assets/kenney_new/modules_offsets.json'\n",
    "middle_x = 0\n",
    "middle_y = 0\n",
    "if os.path.exists(modules_offsets_file):\n",
    "    with open(modules_offsets_file, 'r') as f:\n",
    "        offsets = json.load(f)\n",
    "    f.close()\n",
    "for class_name, class_offsets in offsets.items():\n",
    "    for style, offset in class_offsets.items():\n",
    "        offsets[class_name][style] = (offset[0]+middle_x, offset[1]+middle_y)\n",
    "#save the offsets to a file\n",
    "with open(modules_offsets_file, 'w') as f:\n",
    "    json.dump(offsets, f, indent=4)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa697f7",
   "metadata": {},
   "source": [
    "### Utils functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5da52cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flip_images(images, output_dir):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    for image in images:\n",
    "        output_path = f\"{output_dir}/{os.path.basename(image)}\"\n",
    "        img = Image.open(image)\n",
    "        flipped_img = img.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "        flipped_img.save(output_path)\n",
    "        print(f'Flipped image saved: {image.replace(\".png\", \"_flipped.png\")}')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b32e2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_paths_by_order(paths:list[str], order:list[str]) -> list[str]:\n",
    "    \"\"\"\n",
    "    Sorts the list of paths based on the predefined order.\n",
    "    \"\"\"\n",
    "    order_dict = {name: index for index, name in enumerate(order)}\n",
    "    return sorted(paths, key=lambda path: order_dict.get(path.split('/')[-2], float('inf')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627f2946",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_composents(image_paths: list[str], output_path:str, save=True, output_size = None) -> Image:\n",
    "    offsets = json.load(open('assets/kenney_new/modules_offsets.json', 'r'))\n",
    "    \n",
    "    \n",
    "    image_paths = sort_paths_by_order(image_paths, MODULES_ORDER)\n",
    "    images = [Image.open(path) for path in image_paths]\n",
    "    # Calculate the width and height of the merged image\n",
    "    total_width = 600\n",
    "    total_height = 600\n",
    "    middle_x = 0\n",
    "    middle_y = 0\n",
    "\n",
    "    # Create a new image with the appropriate size\n",
    "    merged_image = Image.new('RGBA', (total_width, total_height))\n",
    "\n",
    "    # Paste each image into the merged image\n",
    "    for i, img in enumerate(images):\n",
    "        class_name = image_paths[i].split('/')[-2]\n",
    "        special_type = os.path.basename(image_paths[i]).split('.')[0].split('_')[-1]\n",
    "        try:\n",
    "            class_offsets = offsets.get(class_name, {'standard':(0, 0)})\n",
    "            x_offset, y_offset = class_offsets.get(special_type, class_offsets['standard'])\n",
    "        except Exception as e:\n",
    "            print(f\"Error with the offset file (should contain a 'standard' value for each class) : {e}\")\n",
    "            x_offset, y_offset = 0, 0\n",
    "            continue\n",
    "        merged_image.alpha_composite(img, (x_offset + middle_x, y_offset + middle_y))\n",
    "    # Save the merged image\n",
    "    if output_size != None:\n",
    "        merged_image = merged_image.resize((output_size,output_size))\n",
    "    if save:\n",
    "        output_dir = os.path.dirname(os.path.abspath(output_path))\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "        merged_image.save(output_path)\n",
    "        \n",
    "    return merged_image\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac512771",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_character(assets_dir: str, output_path: str, logic=True, save=True) -> Image:\n",
    "    \"\"\"\n",
    "    Generates a random character by merging components from the assets directory.\n",
    "    \"\"\"\n",
    "    image_paths = []\n",
    "    classes = [d for d in os.listdir(assets_dir) if os.path.isdir(os.path.join(assets_dir,d))]\n",
    "\n",
    "    if logic:\n",
    "        head = np.random.choice([f'{assets_dir}/Head/{f}' for f in os.listdir(f'{assets_dir}/Head') if f.endswith('.png')])\n",
    "        tint = os.path.basename(head).split('_')[0]\n",
    "        arm_L = f'{assets_dir}/Arm_L/{tint}_arm.png'\n",
    "        arm_R = f'{assets_dir}/Arm_R/{tint}_arm.png'\n",
    "        neck = f'{assets_dir}/Neck/{tint}_neck.png'\n",
    "        hand_L = f'{assets_dir}/Hand_L/{tint}_hand.png'\n",
    "        hand_R = f'{assets_dir}/Hand_R/{tint}_hand.png'\n",
    "        leg_L = f'{assets_dir}/Leg_L/{tint}_leg.png'\n",
    "        leg_R = f'{assets_dir}/Leg_R/{tint}_leg.png'\n",
    "        \n",
    "        pants_L = np.random.choice([f'{assets_dir}/Pants_L/{f}' for f in os.listdir(f'{assets_dir}/Pants_L') if f.endswith('.png')])\n",
    "        pants_R = f'{assets_dir}/Pants_R/{os.path.basename(pants_L)}'\n",
    "        pants_color = os.path.basename(pants_L).split('_')[0]\n",
    "        pants = np.random.choice([f'{assets_dir}/Pants/{f}' for f in os.listdir(f'{assets_dir}/Pants') if f.endswith('.png') and pants_color in f])\n",
    "        \n",
    "        shirt_L = np.random.choice([f'{assets_dir}/Shirt_L/{f}' for f in os.listdir(f'{assets_dir}/Shirt_L') if f.endswith('.png')])\n",
    "        shirt_R = f'{assets_dir}/Shirt_R/{os.path.basename(shirt_L)}'\n",
    "        shirt_color = os.path.basename(shirt_L).split('_')[0][:-3]\n",
    "        shirt = np.random.choice([f'{assets_dir}/Shirt/{f}' for f in os.listdir(f'{assets_dir}/Shirt') if f.endswith('.png') and shirt_color in f])\n",
    "        \n",
    "        shoe_L = np.random.choice([f'{assets_dir}/Shoes_L/{f}' for f in os.listdir(f'{assets_dir}/Shoes_L') if f.endswith('.png')])\n",
    "        shoe_R = f'{assets_dir}/Shoes_R/{os.path.basename(shoe_L)}'\n",
    "        \n",
    "        hair = np.random.choice([f'{assets_dir}/Hair/{f}' for f in os.listdir(f'{assets_dir}/Hair') if f.endswith('.png')])\n",
    "        face = np.random.choice([f'{assets_dir}/Face/{f}' for f in os.listdir(f'{assets_dir}/Face') if f.endswith('.png')])\n",
    "        image_paths = [head, arm_L, arm_R, neck, leg_L, leg_R, hand_L, hand_R, pants, pants_L, pants_R, shirt, shirt_L, shirt_R, shoe_L, shoe_R, hair, face]\n",
    "    else: \n",
    "        for class_name in classes:\n",
    "            class_dir = f'{assets_dir}/{class_name}'\n",
    "            if os.path.isdir(class_dir):\n",
    "                images = [f'{class_dir}/{f}' for f in os.listdir(class_dir) if f.endswith('.png')]\n",
    "                if images:\n",
    "                    image_paths.append(np.random.choice(images))  # Take the first image from each class directory\n",
    "    assert len(image_paths) == len(classes)                \n",
    "    merged_image = merge_composents(image_paths, output_path, save=save)\n",
    "    return merged_image, image_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3b7d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset(assets_dir:str, output_path):\n",
    "    characters = []\n",
    "    \n",
    "    heads = [f'{assets_dir}/Head/{f}' for f in os.listdir(f'{assets_dir}/Head') if f.endswith('.png')]\n",
    "    hairs = [f'{assets_dir}/Hair/{f}' for f in os.listdir(f'{assets_dir}/Hair') if f.endswith('.png')][0:15]\n",
    "    faces = [f'{assets_dir}/Face/{f}' for f in os.listdir(f'{assets_dir}/Face') if f.endswith('.png')][0:1]\n",
    "    shoes = [f'{assets_dir}/Shoes_L/{f}' for f in os.listdir(f'{assets_dir}/Shoes_L') if f.endswith('.png')][0:1]\n",
    "    pants_L = [f'{assets_dir}/Pants_L/{f}' for f in os.listdir(f'{assets_dir}/Pants_L') if f.endswith('.png')]\n",
    "    shirts_L = [f'{assets_dir}/Shirt_L/{f}' for f in os.listdir(f'{assets_dir}/Shirt_L') if f.endswith('.png')]\n",
    "    \n",
    "    for head in tqdm.tqdm(heads, desc='Heads :'):\n",
    "        tint = os.path.basename(head).split('_')[0]\n",
    "        arm_L = f'{assets_dir}/Arm_L/{tint}_arm.png'\n",
    "        arm_R = f'{assets_dir}/Arm_R/{tint}_arm.png'\n",
    "        neck = f'{assets_dir}/Neck/{tint}_neck.png'\n",
    "        hand_L = f'{assets_dir}/Hand_L/{tint}_hand.png'\n",
    "        hand_R = f'{assets_dir}/Hand_R/{tint}_hand.png'\n",
    "        leg_L = f'{assets_dir}/Leg_L/{tint}_leg.png'\n",
    "        leg_R = f'{assets_dir}/Leg_R/{tint}_leg.png'\n",
    "        \n",
    "        for hair in tqdm.tqdm(hairs, desc='Hairs :'):\n",
    "            for face in faces:\n",
    "                for shoe_L in shoes:\n",
    "                    shoe_R = f'{assets_dir}/Shoes_R/{os.path.basename(shoe_L)}'\n",
    "                    \n",
    "                    for pant_L in pants_L:\n",
    "                        pant_R = f'{assets_dir}/Pants_R/{os.path.basename(pant_L)}'\n",
    "                        pants_color = os.path.basename(pant_L).split('_')[0]\n",
    "                        pants = [f'{assets_dir}/Pants/{f}' for f in os.listdir(f'{assets_dir}/Pants') if f.endswith('.png') and pants_color in f]\n",
    "                        for pant in pants:\n",
    "                            for shirt_L in shirts_L:\n",
    "                                shirt_R = f'{assets_dir}/Shirt_R/{os.path.basename(shirt_L)}'\n",
    "                                shirt_color = os.path.basename(shirt_L).split('_')[0][:-3]\n",
    "                                shirts = [f'{assets_dir}/Shirt/{f}' for f in os.listdir(f'{assets_dir}/Shirt') if f.endswith('.png') and shirt_color in f][0:1]\n",
    "                                for shirt in shirts:\n",
    "                                    image_paths = [head, arm_L, arm_R, neck, leg_L, leg_R, hand_L, hand_R, pant, pant_L, pant_R, shirt, shirt_L, shirt_R, shoe_L, shoe_R, hair, face]\n",
    "                                    image_paths = sort_paths_by_order(image_paths, order=MODULES_ORDER)\n",
    "                                    # print(len(heads)*len(hairs)*len(shoes)*len(pants_L)*len(pants)*len(shirts_L)*len(shirts)*len(faces))\n",
    "                                    characters.append(image_paths)\n",
    "                                     \n",
    "    with open(output_path,mode='w') as f:\n",
    "        json.dump(characters,f, indent=4)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ec6b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_from_path(path):\n",
    "    class_name = path.split('/')[-2]\n",
    "    basename = os.path.basename(path).split('.')[0]\n",
    "    if class_name == 'Arm_L':\n",
    "        return f'{basename.split(\"_\")[0].capitalize()} Left Arm'\n",
    "    if class_name == 'Arm_R':\n",
    "        return f'{basename.split(\"_\")[0].capitalize()} Right Arm'\n",
    "    if class_name == 'Head':\n",
    "        return f'{basename.split(\"_\")[0].capitalize()} Head'\n",
    "    if class_name == 'Neck':\n",
    "        return f'{basename.split(\"_\")[0].capitalize()} Neck'\n",
    "    if class_name == 'Leg_L':\n",
    "        return f'{basename.split(\"_\")[0].capitalize()} Left Leg'\n",
    "    if class_name == 'Leg_R':\n",
    "        return f'{basename.split(\"_\")[0].capitalize()} Right Leg'\n",
    "    if class_name == 'Hand_L':\n",
    "        return f'{basename.split(\"_\")[0].capitalize()} Left Hand'\n",
    "    if class_name == 'Hand_R':\n",
    "        return f'{basename.split(\"_\")[0].capitalize()} Right Hand'\n",
    "    if class_name == 'Shirt':\n",
    "        return f'{basename.split(\"_\")[0][:-5].capitalize()} Shirt'\n",
    "    if class_name == 'Shirt_L':\n",
    "        color, size = basename.split(\"_\")\n",
    "        return f'{color[:-3].capitalize()} {size.capitalize()} Left Shirt'\n",
    "    if class_name == 'Shirt_L':\n",
    "        color, size = basename.split(\"_\")\n",
    "        return f'{color[:-3].capitalize()} {size.capitalize()} Right Shirt'\n",
    "    if class_name == 'Shoes_L':\n",
    "        return f'{basename.split(\".\")[0][:-5].capitalize()} Left Shoe'\n",
    "    if class_name == 'Shoes_R':\n",
    "        return f'{basename.split(\".\")[0][:-5].capitalize()} Right Shoe'\n",
    "    if class_name == 'Pants_L':\n",
    "        color, size = basename.split(\"_\")\n",
    "        return f'{color[5:].capitalize()} {size.capitalize()} Left Pant'\n",
    "    if class_name == 'Pants_R':\n",
    "        color, size = basename.split(\"_\")\n",
    "        return f'{color[5:].capitalize()} {size.capitalize()} Right Pant'\n",
    "    if class_name == 'Pants':\n",
    "        return f'{basename[5:].capitalize()} Pants'\n",
    "    if class_name == 'Face':\n",
    "        return 'Face'\n",
    "    if class_name == 'Hair':\n",
    "        color, style = basename.split('_')\n",
    "        return f'{color.capitalize()} {style.capitalize()} Hair'\n",
    "    return basename\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0eb79bb",
   "metadata": {},
   "source": [
    "### Generating path dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ab000c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = f'{assets_dir}/Dataset.json'\n",
    "generate_dataset(assets_dir=assets_dir,output_path=dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef4af0c",
   "metadata": {},
   "source": [
    "### Generating next_token dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ade8a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = json.load(open(f'characters.json',mode='r'))\n",
    "print(f'Loaded dataset with {len(dataset)} charaters of {len(dataset[0])} modules')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe938d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_token_dataset_path = 'dataset.csv'\n",
    "output_dir_next_token = 'kenney_modular_characters'\n",
    "\n",
    "    \n",
    "target = pd.read_csv(next_token_dataset_path)['Target'].to_list()\n",
    "\n",
    "print(len(target)//18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8aca037",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "output_dir_next_token = 'kenney_modular_characters'\n",
    "next_token_dataset_path = 'dataset.csv'\n",
    "\n",
    "IMAGE_SIZE = 128\n",
    "resume = True\n",
    "blank_image = f'{output_dir_next_token}/char_0-layer_0.png'\n",
    "_ = merge_composents([], output_path=blank_image, output_size=IMAGE_SIZE)\n",
    "last_one = 0\n",
    "\n",
    "if resume:\n",
    "    target = pd.read_csv(next_token_dataset_path)['Target'].to_list()\n",
    "    last_one = int(os.path.basename(target[-1]).split('-')[0].split('_')[1])\n",
    "    print(f\"Resuming to character number {last_one} ...\")\n",
    "\n",
    "for char_id, character in tqdm.tqdm(enumerate(dataset, start = last_one), initial=last_one, desc=\"Generating sequence\", total=len(dataset),miniters=10):\n",
    "    if resume:\n",
    "        \n",
    "        output_path = f'{output_dir_next_token}/char_{char_id+1}-layer_18.png'\n",
    "        \n",
    "    previous_path = blank_image\n",
    "    for layer_id in range(1,len(character)+1):\n",
    "        output_path = f'{output_dir_next_token}/char_{char_id+1}-layer_{layer_id}.png'\n",
    "        if not os.path.exists(output_path):\n",
    "            _ = merge_composents(character[:layer_id], output_path=output_path, output_size = IMAGE_SIZE)\n",
    "        row = [previous_path, output_path, get_class_from_path(character[layer_id-1])]\n",
    "        rows.append(row)\n",
    "        previous_path = output_path\n",
    "    if char_id%1000 == 0:\n",
    "        print(f'Saving the dataset ({char_id}/{len(dataset)})')\n",
    "        df_batch = pd.DataFrame(rows, columns=['Input', 'Target', 'Prompt'])\n",
    "        if char_id > 0:\n",
    "            existing_df = pd.read_csv(next_token_dataset_path)\n",
    "            df_batch = pd.concat([existing_df,df_batch], ignore_index=True)\n",
    "        df_batch.to_csv(next_token_dataset_path,index=False) \n",
    "        rows = []\n",
    "        \n",
    "print(f'Saving the dataset (COMPLETE)')\n",
    "df_batch = pd.DataFrame(rows, columns=['Input', 'Target', 'Prompt'])\n",
    "existing_df = pd.read_csv(next_token_dataset_path)\n",
    "df_batch = pd.concat([existing_df,df_batch], ignore_index=True)\n",
    "df_batch.to_csv(next_token_dataset_path,index=False) \n",
    "rows = []\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4c04f9",
   "metadata": {},
   "source": [
    "### Tests cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9924ee3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(next_token_dataset_path)\n",
    "inputs = df['Input'].to_list()\n",
    "targets = df['Target'].to_list()\n",
    "prompts = df['Prompt'].to_list()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe60b580",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = np.random.randint(len(inputs))\n",
    "input_image = Image.open(inputs[index])\n",
    "target_image = Image.open(targets[index])\n",
    "display(input_image)\n",
    "display(target_image)\n",
    "print(prompts[index])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
