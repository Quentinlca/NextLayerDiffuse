{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0aaccfe4",
   "metadata": {},
   "source": [
    "### Imports and constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983434d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import json\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "output_dir = 'kenney_modular_characters'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "assets_dir = 'assets/kenney_new'\n",
    "next_token_dataset_path = 'modular_characters/dataset.csv'\n",
    "output_dir_next_token = 'modular_characters/kenney_modular_characters'\n",
    "MODULES_ORDER = [\n",
    "    'Arm_L',\n",
    "    'Arm_R',\n",
    "    'Neck',\n",
    "    'Head',\n",
    "    'Hand_L',\n",
    "    'Hand_R',\n",
    "    'Shirt',\n",
    "    'Shirt_L',\n",
    "    'Shirt_R',\n",
    "    'Leg_L',\n",
    "    'Leg_R',\n",
    "    'Shoes_L',\n",
    "    'Shoes_R',\n",
    "    'Pants_L',\n",
    "    'Pants_R',\n",
    "    'Pants',\n",
    "    'Face',\n",
    "    'Hair'\n",
    "    ]\n",
    "\n",
    "MAX_FILES_PER_DIR = 1000\n",
    "IMAGE_SIZE = 128\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87b1311",
   "metadata": {},
   "source": [
    "### To update all the offsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4208a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "modules_offsets_file = 'assets/kenney_new/modules_offsets.json'\n",
    "middle_x = 0\n",
    "middle_y = 0\n",
    "if os.path.exists(modules_offsets_file):\n",
    "    with open(modules_offsets_file, 'r') as f:\n",
    "        offsets = json.load(f)\n",
    "    f.close()\n",
    "for class_name, class_offsets in offsets.items():\n",
    "    for style, offset in class_offsets.items():\n",
    "        offsets[class_name][style] = (offset[0]+middle_x, offset[1]+middle_y)\n",
    "#save the offsets to a file\n",
    "with open(modules_offsets_file, 'w') as f:\n",
    "    json.dump(offsets, f, indent=4)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa697f7",
   "metadata": {},
   "source": [
    "### Utils functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5da52cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flip_images(images, output_dir):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    for image in images:\n",
    "        output_path = f\"{output_dir}/{os.path.basename(image)}\"\n",
    "        img = Image.open(image)\n",
    "        flipped_img = img.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "        flipped_img.save(output_path)\n",
    "        print(f'Flipped image saved: {image.replace(\".png\", \"_flipped.png\")}')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b32e2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_paths_by_order(paths:list[str], order:list[str]) -> list[str]:\n",
    "    \"\"\"\n",
    "    Sorts the list of paths based on the predefined order.\n",
    "    \"\"\"\n",
    "    order_dict = {name: index for index, name in enumerate(order)}\n",
    "    return sorted(paths, key=lambda path: order_dict.get(path.split('/')[-2], float('inf')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627f2946",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_composents(modules_paths: list[str], output_dir: str, output_name:str, save=True, output_size = None):\n",
    "    offsets = json.load(open('assets/kenney_new/modules_offsets.json', 'r'))\n",
    "    output_path = None\n",
    "    def find_output_path(_output_dir:str=output_dir, _output_name:str=output_name, _dir_size:int=MAX_FILES_PER_DIR)->str:\n",
    "        # Find output_path:\n",
    "        if not os.path.exists(_output_dir):\n",
    "            os.mkdir(_output_dir)\n",
    "        subdirectories = [d for d in os.listdir(_output_dir) if os.path.isdir(f'{_output_dir}/{d}')]\n",
    "        if not subdirectories:\n",
    "            os.mkdir(f'{_output_dir}/0')\n",
    "            output_path = f'{_output_dir}/0/{_output_name}'\n",
    "        else:\n",
    "            # Last directory : \n",
    "            subdirectories.sort()\n",
    "            last_dir = subdirectories[-1]\n",
    "            if len([f for f in os.listdir(f'{_output_dir}/{last_dir}') if os.path.isfile(f)]) < _dir_size:\n",
    "                output_path = f'{_output_dir}/{last_dir}/{_output_name}'\n",
    "            else:\n",
    "                sub_dir = f'{_output_dir}/{len(subdirectories)}'\n",
    "                os.mkdir(sub_dir)\n",
    "                output_path = f'{sub_dir}/{_output_name}'\n",
    "        return output_path\n",
    "        \n",
    "    \n",
    "    modules_paths = sort_paths_by_order(modules_paths, MODULES_ORDER)\n",
    "    images = [Image.open(path) for path in modules_paths]\n",
    "    # Calculate the width and height of the merged image\n",
    "    total_width = 600\n",
    "    total_height = 600\n",
    "    middle_x = 0\n",
    "    middle_y = 0\n",
    "\n",
    "    # Create a new image with the appropriate size\n",
    "    merged_image = Image.new('RGBA', (total_width, total_height))\n",
    "\n",
    "    # Paste each image into the merged image\n",
    "    for i, img in enumerate(images):\n",
    "        class_name = modules_paths[i].split('/')[-2]\n",
    "        special_type = os.path.basename(modules_paths[i]).split('.')[0].split('_')[-1]\n",
    "        try:\n",
    "            class_offsets = offsets.get(class_name, {'standard':(0, 0)})\n",
    "            x_offset, y_offset = class_offsets.get(special_type, class_offsets['standard'])\n",
    "        except Exception as e:\n",
    "            print(f\"Error with the offset file (should contain a 'standard' value for each class) : {e}\")\n",
    "            x_offset, y_offset = 0, 0\n",
    "            continue\n",
    "        merged_image.alpha_composite(img, (x_offset + middle_x, y_offset + middle_y))\n",
    "    # Save the merged image\n",
    "    if output_size != None:\n",
    "        merged_image = merged_image.resize((output_size,output_size))\n",
    "    if save:\n",
    "        output_path = find_output_path(_output_dir=output_dir,\n",
    "                                       _output_name=output_name,\n",
    "                                       _dir_size=MAX_FILES_PER_DIR)\n",
    "        merged_image.save(output_path)\n",
    "        \n",
    "    return merged_image, output_path\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac512771",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_character(assets_dir: str, output_dir:str ,output_name: str, logic=True, save=True):\n",
    "    \"\"\"\n",
    "    Generates a random character by merging components from the assets directory.\n",
    "    \"\"\"\n",
    "    image_paths = []\n",
    "    classes = [d for d in os.listdir(assets_dir) if os.path.isdir(os.path.join(assets_dir,d))]\n",
    "\n",
    "    if logic:\n",
    "        head = np.random.choice([f'{assets_dir}/Head/{f}' for f in os.listdir(f'{assets_dir}/Head') if f.endswith('.png')])\n",
    "        tint = os.path.basename(head).split('_')[0]\n",
    "        arm_L = f'{assets_dir}/Arm_L/{tint}_arm.png'\n",
    "        arm_R = f'{assets_dir}/Arm_R/{tint}_arm.png'\n",
    "        neck = f'{assets_dir}/Neck/{tint}_neck.png'\n",
    "        hand_L = f'{assets_dir}/Hand_L/{tint}_hand.png'\n",
    "        hand_R = f'{assets_dir}/Hand_R/{tint}_hand.png'\n",
    "        leg_L = f'{assets_dir}/Leg_L/{tint}_leg.png'\n",
    "        leg_R = f'{assets_dir}/Leg_R/{tint}_leg.png'\n",
    "        \n",
    "        pants_L = np.random.choice([f'{assets_dir}/Pants_L/{f}' for f in os.listdir(f'{assets_dir}/Pants_L') if f.endswith('.png')])\n",
    "        pants_R = f'{assets_dir}/Pants_R/{os.path.basename(pants_L)}'\n",
    "        pants_color = os.path.basename(pants_L).split('_')[0]\n",
    "        pants = np.random.choice([f'{assets_dir}/Pants/{f}' for f in os.listdir(f'{assets_dir}/Pants') if f.endswith('.png') and pants_color in f])\n",
    "        \n",
    "        shirt_L = np.random.choice([f'{assets_dir}/Shirt_L/{f}' for f in os.listdir(f'{assets_dir}/Shirt_L') if f.endswith('.png')])\n",
    "        shirt_R = f'{assets_dir}/Shirt_R/{os.path.basename(shirt_L)}'\n",
    "        shirt_color = os.path.basename(shirt_L).split('_')[0][:-3]\n",
    "        shirt = np.random.choice([f'{assets_dir}/Shirt/{f}' for f in os.listdir(f'{assets_dir}/Shirt') if f.endswith('.png') and shirt_color in f])\n",
    "        \n",
    "        shoe_L = np.random.choice([f'{assets_dir}/Shoes_L/{f}' for f in os.listdir(f'{assets_dir}/Shoes_L') if f.endswith('.png')])\n",
    "        shoe_R = f'{assets_dir}/Shoes_R/{os.path.basename(shoe_L)}'\n",
    "        \n",
    "        hair = np.random.choice([f'{assets_dir}/Hair/{f}' for f in os.listdir(f'{assets_dir}/Hair') if f.endswith('.png')])\n",
    "        face = np.random.choice([f'{assets_dir}/Face/{f}' for f in os.listdir(f'{assets_dir}/Face') if f.endswith('.png')])\n",
    "        image_paths = [head, arm_L, arm_R, neck, leg_L, leg_R, hand_L, hand_R, pants, pants_L, pants_R, shirt, shirt_L, shirt_R, shoe_L, shoe_R, hair, face]\n",
    "    else: \n",
    "        for class_name in classes:\n",
    "            class_dir = f'{assets_dir}/{class_name}'\n",
    "            if os.path.isdir(class_dir):\n",
    "                images = [f'{class_dir}/{f}' for f in os.listdir(class_dir) if f.endswith('.png')]\n",
    "                if images:\n",
    "                    image_paths.append(np.random.choice(images))  # Take the first image from each class directory\n",
    "    assert len(image_paths) == len(classes)                \n",
    "    merged_image, output_path = merge_composents(modules_paths=image_paths, output_dir=output_dir, output_name=output_name, save=save)\n",
    "    return merged_image, image_paths, output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3b7d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset(assets_dir:str, output_path):\n",
    "    characters = []\n",
    "    \n",
    "    heads = [f'{assets_dir}/Head/{f}' for f in os.listdir(f'{assets_dir}/Head') if f.endswith('.png')]\n",
    "    hairs = [f'{assets_dir}/Hair/{f}' for f in os.listdir(f'{assets_dir}/Hair') if f.endswith('.png')][0:15]\n",
    "    faces = [f'{assets_dir}/Face/{f}' for f in os.listdir(f'{assets_dir}/Face') if f.endswith('.png')][0:1]\n",
    "    shoes = [f'{assets_dir}/Shoes_L/{f}' for f in os.listdir(f'{assets_dir}/Shoes_L') if f.endswith('.png')][0:1]\n",
    "    pants_L = [f'{assets_dir}/Pants_L/{f}' for f in os.listdir(f'{assets_dir}/Pants_L') if f.endswith('.png')]\n",
    "    shirts_L = [f'{assets_dir}/Shirt_L/{f}' for f in os.listdir(f'{assets_dir}/Shirt_L') if f.endswith('.png')]\n",
    "    \n",
    "    for head in tqdm.tqdm(heads, desc='Heads :'):\n",
    "        tint = os.path.basename(head).split('_')[0]\n",
    "        arm_L = f'{assets_dir}/Arm_L/{tint}_arm.png'\n",
    "        arm_R = f'{assets_dir}/Arm_R/{tint}_arm.png'\n",
    "        neck = f'{assets_dir}/Neck/{tint}_neck.png'\n",
    "        hand_L = f'{assets_dir}/Hand_L/{tint}_hand.png'\n",
    "        hand_R = f'{assets_dir}/Hand_R/{tint}_hand.png'\n",
    "        leg_L = f'{assets_dir}/Leg_L/{tint}_leg.png'\n",
    "        leg_R = f'{assets_dir}/Leg_R/{tint}_leg.png'\n",
    "        \n",
    "        for hair in tqdm.tqdm(hairs, desc='Hairs :'):\n",
    "            for face in faces:\n",
    "                for shoe_L in shoes:\n",
    "                    shoe_R = f'{assets_dir}/Shoes_R/{os.path.basename(shoe_L)}'\n",
    "                    \n",
    "                    for pant_L in pants_L:\n",
    "                        pant_R = f'{assets_dir}/Pants_R/{os.path.basename(pant_L)}'\n",
    "                        pants_color = os.path.basename(pant_L).split('_')[0]\n",
    "                        pants = [f'{assets_dir}/Pants/{f}' for f in os.listdir(f'{assets_dir}/Pants') if f.endswith('.png') and pants_color in f]\n",
    "                        for pant in pants:\n",
    "                            for shirt_L in shirts_L:\n",
    "                                shirt_R = f'{assets_dir}/Shirt_R/{os.path.basename(shirt_L)}'\n",
    "                                shirt_color = os.path.basename(shirt_L).split('_')[0][:-3]\n",
    "                                shirts = [f'{assets_dir}/Shirt/{f}' for f in os.listdir(f'{assets_dir}/Shirt') if f.endswith('.png') and shirt_color in f][0:1]\n",
    "                                for shirt in shirts:\n",
    "                                    image_paths = [head, arm_L, arm_R, neck, leg_L, leg_R, hand_L, hand_R, pant, pant_L, pant_R, shirt, shirt_L, shirt_R, shoe_L, shoe_R, hair, face]\n",
    "                                    image_paths = sort_paths_by_order(image_paths, order=MODULES_ORDER)\n",
    "                                    # print(len(heads)*len(hairs)*len(shoes)*len(pants_L)*len(pants)*len(shirts_L)*len(shirts)*len(faces))\n",
    "                                    characters.append(image_paths)\n",
    "                                     \n",
    "    with open(output_path,mode='w') as f:\n",
    "        json.dump(characters,f, indent=4)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ec6b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_from_path(path):\n",
    "    class_name = path.split('/')[-2]\n",
    "    basename = os.path.basename(path).split('.')[0]\n",
    "    if class_name == 'Arm_L':\n",
    "        return f'{basename.split(\"_\")[0].capitalize()} Left Arm'\n",
    "    if class_name == 'Arm_R':\n",
    "        return f'{basename.split(\"_\")[0].capitalize()} Right Arm'\n",
    "    if class_name == 'Head':\n",
    "        return f'{basename.split(\"_\")[0].capitalize()} Head'\n",
    "    if class_name == 'Neck':\n",
    "        return f'{basename.split(\"_\")[0].capitalize()} Neck'\n",
    "    if class_name == 'Leg_L':\n",
    "        return f'{basename.split(\"_\")[0].capitalize()} Left Leg'\n",
    "    if class_name == 'Leg_R':\n",
    "        return f'{basename.split(\"_\")[0].capitalize()} Right Leg'\n",
    "    if class_name == 'Hand_L':\n",
    "        return f'{basename.split(\"_\")[0].capitalize()} Left Hand'\n",
    "    if class_name == 'Hand_R':\n",
    "        return f'{basename.split(\"_\")[0].capitalize()} Right Hand'\n",
    "    if class_name == 'Shirt':\n",
    "        return f'{basename.split(\"_\")[0][:-5].capitalize()} Shirt'\n",
    "    if class_name == 'Shirt_L':\n",
    "        color, size = basename.split(\"_\")\n",
    "        return f'{color[:-3].capitalize()} {size.capitalize()} Left Shirt'\n",
    "    if class_name == 'Shirt_R':\n",
    "        color, size = basename.split(\"_\")\n",
    "        return f'{color[:-3].capitalize()} {size.capitalize()} Right Shirt'\n",
    "    if class_name == 'Shoes_L':\n",
    "        return f'{basename.split(\".\")[0][:-5].capitalize()} Left Shoe'\n",
    "    if class_name == 'Shoes_R':\n",
    "        return f'{basename.split(\".\")[0][:-5].capitalize()} Right Shoe'\n",
    "    if class_name == 'Pants_L':\n",
    "        color, size = basename.split(\"_\")\n",
    "        return f'{color[5:].capitalize()} {size.capitalize()} Left Pant'\n",
    "    if class_name == 'Pants_R':\n",
    "        color, size = basename.split(\"_\")\n",
    "        return f'{color[5:].capitalize()} {size.capitalize()} Right Pant'\n",
    "    if class_name == 'Pants':\n",
    "        return f'{basename[5:].capitalize()} Pants'\n",
    "    if class_name == 'Face':\n",
    "        return 'Face'\n",
    "    if class_name == 'Hair':\n",
    "        color, style = basename.split('_')\n",
    "        return f'{color.capitalize()} {style.capitalize()} Hair'\n",
    "    return basename\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0eb79bb",
   "metadata": {},
   "source": [
    "### Generating path dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ab000c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = f'{assets_dir}/Dataset.json'\n",
    "generate_dataset(assets_dir=assets_dir,output_path=dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef4af0c",
   "metadata": {},
   "source": [
    "### Generating next_token dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ade8a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = json.load(open(f'characters.json',mode='r'))\n",
    "print(f'Loaded dataset with {len(dataset)} charaters of {len(dataset[0])} modules')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8aca037",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "\n",
    "resume = True\n",
    "blank_image_name = f'char_0000000-layer_0.png'\n",
    "blank_image_path = f'{output_dir_next_token}/0/{blank_image_name}'\n",
    "if not os.path.exists(blank_image_path):\n",
    "    _, blank_image_path= merge_composents([], output_dir=output_dir_next_token, output_name=blank_image_name, output_size=IMAGE_SIZE)\n",
    "last_one = 0\n",
    "\n",
    "if resume:\n",
    "    target = pd.read_csv(next_token_dataset_path)['Target'].to_list()\n",
    "    last_one = int(os.path.basename(target[-1]).split('-')[0].split('_')[1])\n",
    "    print(f\"Resuming to character number {last_one} ...\")\n",
    "\n",
    "for char_id, character in tqdm.tqdm(enumerate(dataset, start = last_one), initial=last_one, desc=\"Generating sequence\", total=len(dataset),miniters=10):  \n",
    "    previous_path = blank_image_path\n",
    "    for layer_id in range(1, len(character) + 1):\n",
    "        file_number = (char_id)*18 + layer_id + 1\n",
    "        sub_dir_number = file_number//MAX_FILES_PER_DIR\n",
    "        char_number = '0'*(7-len(str(char_id+1)))+str(char_id+1)\n",
    "        output_name = f'char_{char_number}-layer_{layer_id}.png'\n",
    "        output_path = f'{output_dir_next_token}/{sub_dir_number}/{output_name}'\n",
    "        # Check if output_name exists in any subdirectory of output_dir_next_token\n",
    "        if not os.path.exists(output_path):\n",
    "            _, output_path = merge_composents(character[:layer_id], output_dir=output_dir_next_token, output_name=output_name, output_size=IMAGE_SIZE)\n",
    "        row = [previous_path, output_path, get_class_from_path(character[layer_id-1])]\n",
    "        rows.append(row)\n",
    "        previous_path = output_path\n",
    "    if char_id%10000 == 0:\n",
    "        print(f'Saving the dataset ({char_id}/{len(dataset)})')\n",
    "        df_batch = pd.DataFrame(rows, columns=['Input', 'Target', 'Prompt'])\n",
    "        if char_id > 0:\n",
    "            existing_df = pd.read_csv(next_token_dataset_path)\n",
    "            df_batch = pd.concat([existing_df,df_batch], ignore_index=True)\n",
    "        df_batch.to_csv(next_token_dataset_path,index=False) \n",
    "        rows = []\n",
    "        \n",
    "print(f'Saving the dataset (COMPLETE)')\n",
    "df_batch = pd.DataFrame(rows, columns=['Input', 'Target', 'Prompt'])\n",
    "existing_df = pd.read_csv(next_token_dataset_path)\n",
    "df_batch = pd.concat([existing_df,df_batch], ignore_index=True)\n",
    "df_batch.to_csv(next_token_dataset_path,index=False) \n",
    "rows = []\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fda1870",
   "metadata": {},
   "source": [
    "### Upload the dataset to the hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168554d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Generator to yield the dataset\n",
    "# We want the dataset to look like : row = {'input': PIL image, 'target': PIL image, 'prompt': str}\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "def convert_file_names(inputs, targets, prompts):\n",
    "    for input, target, prompt in zip(inputs, targets, prompts):\n",
    "        if not os.path.exists(input) or not os.path.exists(target):\n",
    "            continue\n",
    "        input_image = Image.open(input)\n",
    "        target_image = Image.open(target)\n",
    "        row = {'input': input_image,\n",
    "               'target': target_image,\n",
    "               'prompt': prompt}\n",
    "        \n",
    "        yield row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4bb29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(next_token_dataset_path)\n",
    "inputs = df['Input'].to_list()\n",
    "targets = df['Target'].to_list()\n",
    "prompts = df['Prompt'].to_list()\n",
    "\n",
    "dataset_to_hub = Dataset.from_generator(lambda: convert_file_names(inputs, targets, prompts), cache_dir='cache')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2fe4852",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_to_hub.push_to_hub(repo_id='QLeca/modular_characters')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4c04f9",
   "metadata": {},
   "source": [
    "### Tests cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5e12bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir(output_dir_next_token)\n",
    "files = [f for f in files if f.endswith('.png')]\n",
    "\n",
    "files.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce395a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ff53ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(files)):\n",
    "    current_subdir = i//MAX_FILES_PER_DIR\n",
    "    current_subdir_path = f'{output_dir_next_token}/{current_subdir}'\n",
    "    if not os.path.exists(current_subdir_path):\n",
    "        os.mkdir(current_subdir_path)\n",
    "    assert len(os.listdir(current_subdir_path)) < MAX_FILES_PER_DIR\n",
    "    old_path = f'{output_dir_next_token}/{files[i]}'\n",
    "    new_path = f'{current_subdir_path}/{files[i]}'\n",
    "    os.rename(old_path, new_path)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9924ee3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(next_token_dataset_path)\n",
    "inputs = df['Input'].to_list()\n",
    "targets = df['Target'].to_list()\n",
    "prompts = df['Prompt'].to_list()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe60b580",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = np.random.randint(len(inputs))\n",
    "input_image = Image.open(inputs[index])\n",
    "target_image = Image.open(targets[index])\n",
    "display(input_image)\n",
    "display(target_image)\n",
    "print(prompts[index])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
